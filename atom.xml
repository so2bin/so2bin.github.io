<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>so2bin</title>
  
  <subtitle>青山一道同云雨，明月何曾是两乡</subtitle>
  <link href="https://so2bin.github.io/atom.xml" rel="self"/>
  
  <link href="https://so2bin.github.io/"/>
  <updated>2024-03-03T07:31:31.879Z</updated>
  <id>https://so2bin.github.io/</id>
  
  <author>
    <name>so2bin</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>k8s scheduler</title>
    <link href="https://so2bin.github.io/2024/03/03/cloud-native/k8s-scheduler/"/>
    <id>https://so2bin.github.io/2024/03/03/cloud-native/k8s-scheduler/</id>
    <published>2024-03-03T03:06:37.000Z</published>
    <updated>2024-03-03T07:31:31.879Z</updated>
    
    <content type="html"><![CDATA[<h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><ul><li><a href="https://github.com/rfyiamcool/notes/blob/main/kubernetes_scheduler_code.md">https://github.com/rfyiamcool/notes/blob/main/kubernetes_scheduler_code.md</a></li></ul><h2 id="scheduler原理"><a href="#scheduler原理" class="headerlink" title="scheduler原理"></a>scheduler原理</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;资料&quot;&gt;&lt;a href=&quot;#资料&quot; class=&quot;headerlink&quot; title=&quot;资料&quot;&gt;&lt;/a&gt;资料&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/rfyiamcool/notes/blob/main/kubernete</summary>
      
    
    
    
    
    <category term="k8s" scheme="https://so2bin.github.io/tags/k8s/"/>
    
    <category term="scheduler" scheme="https://so2bin.github.io/tags/scheduler/"/>
    
  </entry>
  
  <entry>
    <title>Tritonserver</title>
    <link href="https://so2bin.github.io/2024/03/03/AI-Infer/Tritonserver/"/>
    <id>https://so2bin.github.io/2024/03/03/AI-Infer/Tritonserver/</id>
    <published>2024-03-03T03:02:05.000Z</published>
    <updated>2024-03-03T03:02:40.597Z</updated>
    
    
    
    
    
    <category term="GPU" scheme="https://so2bin.github.io/tags/GPU/"/>
    
    <category term="Tritonserver" scheme="https://so2bin.github.io/tags/Tritonserver/"/>
    
  </entry>
  
  <entry>
    <title>k8s device-plugin</title>
    <link href="https://so2bin.github.io/2024/03/02/cloud-native/device-plugin/"/>
    <id>https://so2bin.github.io/2024/03/02/cloud-native/device-plugin/</id>
    <published>2024-03-01T16:10:11.000Z</published>
    <updated>2024-03-03T05:06:26.626Z</updated>
    
    <content type="html"><![CDATA[<h2 id="k8s-device-plugin机制"><a href="#k8s-device-plugin机制" class="headerlink" title="k8s device-plugin机制"></a>k8s device-plugin机制</h2><h3 id="docker运行GPU容器"><a href="#docker运行GPU容器" class="headerlink" title="docker运行GPU容器"></a>docker运行GPU容器</h3><ul><li>nvidia的GPU容器镜像原理是：NVIDIA驱动相对更稳定，因此容器中使用容器封装的CUDA&#x2F;SDK库，共用宿主机的NVIDIA驱动；</li><li>docker运行GPU容器时，需要将NVIDIA驱动映射到容器内：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 以下的命令与nvidia-docker同样的作用原理</span></span><br><span class="line">docker run --it --volume=navidia_driver_xxx.xx:/usr/local/nvidia:ro \</span><br><span class="line">    --device=/dev/nvidiactl \</span><br><span class="line">    --device=/dev/nvidia-uvm \</span><br><span class="line">    --device=/dev/nvidia-uvm-tools \</span><br><span class="line">    --device=/dev/nvidia0 \</span><br><span class="line">    nvidia/cuda nvidia-smi</span><br></pre></td></tr></table></figure></li></ul><h3 id="k8s-运行GPU容器"><a href="#k8s-运行GPU容器" class="headerlink" title="k8s 运行GPU容器"></a>k8s 运行GPU容器</h3><ol><li>安装NVIDIA驱动；</li><li>安装NVIDIA Dcoker：<code>nvidia-docker2</code></li><li>部署NVIDIA Device Plugin：<code>device-nvidia-plugin</code></li></ol><h3 id="k8s-GPU资源插件原理"><a href="#k8s-GPU资源插件原理" class="headerlink" title="k8s GPU资源插件原理"></a>k8s GPU资源插件原理</h3><p>利用了两种技术：</p><ol><li>Extended Resources，允许用用户自定义资源扩展，如这里的扩展：<code>nvidia.com/gpu: 2</code>, 也可用于如RDMA, AMD GPU, FPGA等；</li><li>Device Plugin Framework，允许第三方设备提供商以插件外置的方式对设置的调度和全生命周期管理；</li></ol><span id="more"></span><h4 id="Extended-Resources"><a href="#Extended-Resources" class="headerlink" title="Extended Resources"></a>Extended Resources</h4><ul><li><p>属于node级别的资源，是可以与Device Plugin独立的；</p></li><li><p>可以直接通过一个k8s REST API完成node扩展资源的上报：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl --header <span class="string">&quot;Content-Type: application/json-patch+json&quot;</span> \</span><br><span class="line">    --request PATCH \</span><br><span class="line">    --data <span class="string">&#x27;[&#123;&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/status/capacity/nvidia.com/gpu&quot;, &quot;value&quot;: &quot;1&quot;&#125;]&#x27;</span> \</span><br><span class="line">    https://localhost:6443/api/v1/nodes/&lt;target node&gt;/staus</span><br></pre></td></tr></table></figure><p>对应的node资源表现：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get node xxx -o yaml</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">allocatable:</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">&quot;128&quot;</span></span><br><span class="line">    <span class="attr">ephemeral-storage:</span> <span class="string">&quot;6804750049483&quot;</span></span><br><span class="line">    <span class="attr">hugepages-1Gi:</span> <span class="string">&quot;0&quot;</span></span><br><span class="line">    <span class="attr">hugepages-2Mi:</span> <span class="string">&quot;0&quot;</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">527904364Ki</span></span><br><span class="line">    <span class="attr">pods:</span> <span class="string">&quot;110&quot;</span></span><br><span class="line">    <span class="attr">tencent.com/vcuda-core:</span> <span class="string">&quot;800&quot;</span></span><br><span class="line">    <span class="attr">tencent.com/vcuda-memory:</span> <span class="string">&quot;767&quot;</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">&quot;128&quot;</span></span><br><span class="line">    <span class="attr">ephemeral-storage:</span> <span class="string">7383626368Ki</span></span><br><span class="line">    <span class="attr">hugepages-1Gi:</span> <span class="string">&quot;0&quot;</span></span><br><span class="line">    <span class="attr">hugepages-2Mi:</span> <span class="string">&quot;0&quot;</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">528006764Ki</span></span><br><span class="line">    <span class="attr">pods:</span> <span class="string">&quot;110&quot;</span></span><br><span class="line">    <span class="attr">tencent.com/vcuda-core:</span> <span class="string">&quot;800&quot;</span></span><br><span class="line">    <span class="attr">tencent.com/vcuda-memory:</span> <span class="string">&quot;767&quot;</span></span><br></pre></td></tr></table></figure></li><li><p>如果是用的Device Plugin机制实现，则需要手动上报，插件会在设备上报的过程中自动完成；</p></li></ul><h4 id="Device-Plugin机制"><a href="#Device-Plugin机制" class="headerlink" title="Device Plugin机制"></a>Device Plugin机制</h4><h5 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h5><ul><li><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/</a></li></ul><h5 id="Device-Plugin实现"><a href="#Device-Plugin实现" class="headerlink" title="Device Plugin实现"></a>Device Plugin实现</h5><ol><li><p>Initialization: 完成设备相关的初始化工作，设备进入Ready状态；</p></li><li><p>plugin启动一个GRPC service，将通过本地unix socket完成端口监听，实现如下接口：</p><figure class="highlight go"><table><tr><td class="code"><pre><span class="line">service DevicePlugin &#123;</span><br><span class="line">      <span class="comment">// GetDevicePluginOptions returns options to be communicated with Device Manager.</span></span><br><span class="line">      rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) &#123;&#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// ListAndWatch returns a stream of List of Devices</span></span><br><span class="line">      <span class="comment">// Whenever a Device state change or a Device disappears, ListAndWatch</span></span><br><span class="line">      <span class="comment">// returns the new list</span></span><br><span class="line">      rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) &#123;&#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Allocate is called during container creation so that the Device</span></span><br><span class="line">      <span class="comment">// Plugin can run device specific operations and instruct Kubelet</span></span><br><span class="line">      <span class="comment">// of the steps to make the Device available in the container</span></span><br><span class="line">      rpc Allocate(AllocateRequest) returns (AllocateResponse) &#123;&#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// GetPreferredAllocation returns a preferred set of devices to allocate</span></span><br><span class="line">      <span class="comment">// from a list of available ones. The resulting preferred allocation is not</span></span><br><span class="line">      <span class="comment">// guaranteed to be the allocation ultimately performed by the</span></span><br><span class="line">      <span class="comment">// devicemanager. It is only designed to help the devicemanager make a more</span></span><br><span class="line">      <span class="comment">// informed allocation decision when possible.</span></span><br><span class="line">      rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) &#123;&#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// PreStartContainer is called, if indicated by Device Plugin during registration phase,</span></span><br><span class="line">      <span class="comment">// before each container start. Device plugin can run device specific operations</span></span><br><span class="line">      <span class="comment">// such as resetting the device before making devices available to the container.</span></span><br><span class="line">      rpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>其中<code>GetDevicePluginOptions</code>与<code>PreStartContainer</code>的实现是可选的，也就是说如果不需要，实现为空函数即可</p></blockquote></li><li><p>向kubelet发起注册，带上unix socket位置信息；</p></li><li><p>注册成功后，plugin会监控设备的状态，将主动向kubelet上报变化或健康状态(<code>ListAndWatch</code>)，同时还需要处理<code>Allocate</code>请求，在处理分配请求的过程，可能需要执行一些设备相关的初始化操作，并需要将一些必要的信息响应回去，以达到让kubelet对容器运行时参数进行一些修改，如配置容器的Annotations, environments, device nodes, mounts等；</p></li><li><p>plugin还需要能处理kubelet重启的情况，并进行重新注册；</p></li></ol><p><strong>当plugin检测到设备异常时</strong></p><ul><li>plugin会向kubelet上报状态</li><li>如果异常Device处于空闲未分配状态，则kubelet会将其从可分配名额中剔除</li><li>而如果异常Device处于占用状态，kubelet不会做任何处理，因为直接删除pod是不合理的危险操作</li></ul><p><strong>kubelet向APIServer上报数据</strong></p><ul><li>kubelet只会向APIServer上报GPU的数量，不会上报其它信息，如GPU ID列表；</li><li>这些GPU ID列表只会保存在kubelet Device Plugin Manager中；</li><li>该机制就会导致，调度器，只能看到GPU的数量配置，看不到具体的GPU列表，因此无法实现一些复杂场景的调度；</li></ul><h4 id="Pod申请资源调度过程"><a href="#Pod申请资源调度过程" class="headerlink" title="Pod申请资源调度过程"></a>Pod申请资源调度过程</h4><ol><li>k8s scechuler根据资源类型和数量，选择合适的node绑定到pod上，该node的资源数量减少；</li><li>kubelet Device Plugin Manager会向Plugin发起<code>Allocate</code>请求，Plugin会响应相关的参数，如env，mount等，这些参数将会在容器创建过程中对容器进行修改；</li><li>kubelet创建容器，并配置上<code>Allocate</code>响应的参数；</li></ol><h4 id="Device-Plugin集成Topology-Manager"><a href="#Device-Plugin集成Topology-Manager" class="headerlink" title="Device Plugin集成Topology Manager"></a>Device Plugin集成Topology Manager</h4><ul><li>Togology Manager是kubelet的一个组件，允许对资源调度进行拓扑对齐管理；</li><li>为了实现该功能同，Device Plugin API扩展了<code>TopologyInfo</code>结构：<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">message TopologyInfo &#123;</span><br><span class="line">    repeated NUMANode nodes = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">message NUMANode &#123;</span><br><span class="line">    <span class="type">int64</span> ID = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>需要进入拓扑管理的Device Plugin在进入Device注册时，可以带上一个<code>TopologyInfo</code>信息，里面包括Device ID和健康状态，利用这些信息即可由Togology Manager来提供资源分配的拓扑决策；</li></ul><h4 id="当前缺陷"><a href="#当前缺陷" class="headerlink" title="当前缺陷"></a>当前缺陷</h4><ol><li>设备调度发生在kubelet层面，缺乏全局视角</li><li>资源上报信息有限（仅数量），导致精细度不足，如无法解决异构场景的调度</li><li>调度策略简单，并且无法配置，无法应用复杂场景</li></ol><h4 id="社区异构资源调度方案"><a href="#社区异构资源调度方案" class="headerlink" title="社区异构资源调度方案"></a>社区异构资源调度方案</h4><ul><li>Nvidia GPU Device Plugin: <a href="https://github.com/NVIDIA/k8s-device-plugin.git">https://github.com/NVIDIA/k8s-device-plugin.git</a></li><li>GPU Share Device Plugin:<ul><li><a href="https://github.com/AliyunContainerService/gpushare-scheduler-extender.git">https://github.com/AliyunContainerService/gpushare-scheduler-extender.git</a></li><li><a href="https://github.com/AliyunContainerService/gpushare-device-plugin.git">https://github.com/AliyunContainerService/gpushare-device-plugin.git</a></li></ul></li><li>RDMA Device Plugin: <a href="https://github.com/Mellanox/k8s-rdma-shared-dev-plugin.git">https://github.com/Mellanox/k8s-rdma-shared-dev-plugin.git</a></li><li>FPGA Device Plugin: <a href="https://github.com/Xilinx/FPGA_as_a_Service/tree/master/k8s-device-plugin">https://github.com/Xilinx/FPGA_as_a_Service/tree/master/k8s-device-plugin</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;k8s-device-plugin机制&quot;&gt;&lt;a href=&quot;#k8s-device-plugin机制&quot; class=&quot;headerlink&quot; title=&quot;k8s device-plugin机制&quot;&gt;&lt;/a&gt;k8s device-plugin机制&lt;/h2&gt;&lt;h3 id=&quot;docker运行GPU容器&quot;&gt;&lt;a href=&quot;#docker运行GPU容器&quot; class=&quot;headerlink&quot; title=&quot;docker运行GPU容器&quot;&gt;&lt;/a&gt;docker运行GPU容器&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;nvidia的GPU容器镜像原理是：NVIDIA驱动相对更稳定，因此容器中使用容器封装的CUDA&amp;#x2F;SDK库，共用宿主机的NVIDIA驱动；&lt;/li&gt;
&lt;li&gt;docker运行GPU容器时，需要将NVIDIA驱动映射到容器内：&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 以下的命令与nvidia-docker同样的作用原理&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;docker run --it --volume=navidia_driver_xxx.xx:/usr/local/nvidia:ro &#92;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    --device=/dev/nvidiactl &#92;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    --device=/dev/nvidia-uvm &#92;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    --device=/dev/nvidia-uvm-tools &#92;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    --device=/dev/nvidia0 &#92;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    nvidia/cuda nvidia-smi&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;k8s-运行GPU容器&quot;&gt;&lt;a href=&quot;#k8s-运行GPU容器&quot; class=&quot;headerlink&quot; title=&quot;k8s 运行GPU容器&quot;&gt;&lt;/a&gt;k8s 运行GPU容器&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;安装NVIDIA驱动；&lt;/li&gt;
&lt;li&gt;安装NVIDIA Dcoker：&lt;code&gt;nvidia-docker2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;部署NVIDIA Device Plugin：&lt;code&gt;device-nvidia-plugin&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;k8s-GPU资源插件原理&quot;&gt;&lt;a href=&quot;#k8s-GPU资源插件原理&quot; class=&quot;headerlink&quot; title=&quot;k8s GPU资源插件原理&quot;&gt;&lt;/a&gt;k8s GPU资源插件原理&lt;/h3&gt;&lt;p&gt;利用了两种技术：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Extended Resources，允许用用户自定义资源扩展，如这里的扩展：&lt;code&gt;nvidia.com/gpu: 2&lt;/code&gt;, 也可用于如RDMA, AMD GPU, FPGA等；&lt;/li&gt;
&lt;li&gt;Device Plugin Framework，允许第三方设备提供商以插件外置的方式对设置的调度和全生命周期管理；&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    
    <category term="k8s" scheme="https://so2bin.github.io/tags/k8s/"/>
    
    <category term="device-plugin" scheme="https://so2bin.github.io/tags/device-plugin/"/>
    
  </entry>
  
  <entry>
    <title>Go Knowledge</title>
    <link href="https://so2bin.github.io/2024/02/29/Go/Go-Knowledge/"/>
    <id>https://so2bin.github.io/2024/02/29/Go/Go-Knowledge/</id>
    <published>2024-02-29T12:56:05.000Z</published>
    <updated>2024-03-04T12:46:27.247Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Go-init"><a href="#Go-init" class="headerlink" title="Go init"></a>Go init</h2><h3 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h3><ul><li><a href="https://cloud.tencent.com/developer/article/2138066">https://cloud.tencent.com/developer/article/2138066</a></li></ul><h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><ul><li><p>Go的总体初始化过程如下图所示：</p><img src="/2024/02/29/Go/Go-Knowledge/go-init-order.png" width="75%" style="margin: 0 auto;"></li><li><p>同模块中，优先级：const常量 &gt; var变量 &gt; <code>init()</code></p></li><li><p>同模块中，前后<code>init()</code>的优先级，为从上到下顺序执行</p></li><li><p>同包中，不同模块的<code>init()</code>优先级：按文件名排序依次执行不同模块的<code>init()</code></p></li><li><p>import导入包<code>init()</code>顺序：被导入包的<code>init()</code>先执行</p></li></ul><span id="more"></span>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Go-init&quot;&gt;&lt;a href=&quot;#Go-init&quot; class=&quot;headerlink&quot; title=&quot;Go init&quot;&gt;&lt;/a&gt;Go init&lt;/h2&gt;&lt;h3 id=&quot;资料&quot;&gt;&lt;a href=&quot;#资料&quot; class=&quot;headerlink&quot; title=&quot;资料&quot;&gt;&lt;/a&gt;资料&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://cloud.tencent.com/developer/article/2138066&quot;&gt;https://cloud.tencent.com/developer/article/2138066&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;说明&quot;&gt;&lt;a href=&quot;#说明&quot; class=&quot;headerlink&quot; title=&quot;说明&quot;&gt;&lt;/a&gt;说明&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Go的总体初始化过程如下图所示：&lt;/p&gt;
&lt;img src=&quot;/2024/02/29/Go/Go-Knowledge/go-init-order.png&quot; width=&quot;75%&quot; style=&quot;margin: 0 auto;&quot;&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;同模块中，优先级：const常量 &amp;gt; var变量 &amp;gt; &lt;code&gt;init()&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;同模块中，前后&lt;code&gt;init()&lt;/code&gt;的优先级，为从上到下顺序执行&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;同包中，不同模块的&lt;code&gt;init()&lt;/code&gt;优先级：按文件名排序依次执行不同模块的&lt;code&gt;init()&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;import导入包&lt;code&gt;init()&lt;/code&gt;顺序：被导入包的&lt;code&gt;init()&lt;/code&gt;先执行&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="Go" scheme="https://so2bin.github.io/tags/Go/"/>
    
  </entry>
  
  <entry>
    <title>etcd</title>
    <link href="https://so2bin.github.io/2024/02/22/k8s/etcd/"/>
    <id>https://so2bin.github.io/2024/02/22/k8s/etcd/</id>
    <published>2024-02-22T07:15:13.000Z</published>
    <updated>2024-03-01T16:09:42.927Z</updated>
    
    <content type="html"><![CDATA[<h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><ul><li>ETCD事件推送：<a href="https://www.lixueduan.com/posts/etcd/05-watch/">https://www.lixueduan.com/posts/etcd/05-watch/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;资料&quot;&gt;&lt;a href=&quot;#资料&quot; class=&quot;headerlink&quot; title=&quot;资料&quot;&gt;&lt;/a&gt;资料&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;ETCD事件推送：&lt;a href=&quot;https://www.lixueduan.com/posts/etcd/05-watch/</summary>
      
    
    
    
    
    <category term="etcd" scheme="https://so2bin.github.io/tags/etcd/"/>
    
  </entry>
  
  <entry>
    <title>pod</title>
    <link href="https://so2bin.github.io/2024/02/22/k8s/pod/"/>
    <id>https://so2bin.github.io/2024/02/22/k8s/pod/</id>
    <published>2024-02-22T03:59:09.000Z</published>
    <updated>2024-03-01T16:09:42.927Z</updated>
    
    <content type="html"><![CDATA[<h2 id="pod内容器共享空间"><a href="#pod内容器共享空间" class="headerlink" title="pod内容器共享空间"></a>pod内容器共享空间</h2><p><strong>资料</strong></p><ul><li><a href="https://www.cnblogs.com/rexcheny/p/11017146.html">https://www.cnblogs.com/rexcheny/p/11017146.html</a></li><li><a href="https://linchpiner.github.io/k8s-multi-container-pods.html">https://linchpiner.github.io/k8s-multi-container-pods.html</a></li></ul><p><strong>问题</strong></p><p>k8s的pod内可以同时容纳多个容器，那这些容器之间有哪些资源（命名空间）是可以共享的？</p><p><strong>答案</strong></p><p><img src="/2024/02/22/k8s/pod/pod-isolation.png" alt="pod share"></p><ul><li><p>容器的隔离技术是通过cgroup和namespace隔离实现，linux支持namespace有：</p><ul><li>UTS名称空间，保存内核版本，主机名，域名</li><li>NET空间，通过逻辑网络栈实现网络空间的隔离</li><li>进程PID空间，通过fork时指定的一个参数控制，不同空间间的PID是隔离了，看不到彼此的PID，子空间看不到父空间的内容，但父空间可以管理子空间，如发送信息</li><li>IPC空间，即进程间通信的隔离，不同容器之间无法通过如信号量，队列，共享内存（System V IPC和POSIX消息队列），来实现进程间通信</li><li>USER空间，实现用户，组相关功能的隔离</li><li>MNT空间，即磁盘挂载点和文件系统的隔离能力，同一主机上的不同进程访问相同的路径能看到相同的内容，是因为他们共享本机的磁盘和文件系统</li><li>文件系统：不同容器都有自己的一个snapshotter目录，文件系统是隔离的</li></ul></li><li><p>pod内的不同容器之间的共享稍有不同：</p><ul><li>UTS名称空间，是共享的</li><li>USER空间，共享的</li><li>NET空间，是共享的，因此不同容器启相同端口会冲突</li><li>PID空间，默认是关闭的，可以通过<code>shareProcessNamespace: true</code>打开</li><li>IPC空间，是共享的<br>  <img src="/2024/02/22/k8s/pod/pod-c-share-IPC.png" alt="pod-c-share-IPC"></li><li>MNT空间，隔离的，但可以通过挂载同一个pod volume来共享挂载的内容，实现不同容器间的共享挂载，但不同容器之间的文件系统是隔离的<br>  <img src="/2024/02/22/k8s/pod/pod-c-share-volume.png" alt="pod-c-share-volume"></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;pod内容器共享空间&quot;&gt;&lt;a href=&quot;#pod内容器共享空间&quot; class=&quot;headerlink&quot; title=&quot;pod内容器共享空间&quot;&gt;&lt;/a&gt;pod内容器共享空间&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;资料&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a hr</summary>
      
    
    
    
    
    <category term="k8s" scheme="https://so2bin.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>probe</title>
    <link href="https://so2bin.github.io/2024/02/20/k8s/probe/"/>
    <id>https://so2bin.github.io/2024/02/20/k8s/probe/</id>
    <published>2024-02-20T01:49:46.000Z</published>
    <updated>2024-02-21T11:02:07.691Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Readiness-Liveness-StartupProbe"><a href="#Readiness-Liveness-StartupProbe" class="headerlink" title="Readiness, Liveness, StartupProbe"></a>Readiness, Liveness, StartupProbe</h2><h3 id="执行原理"><a href="#执行原理" class="headerlink" title="执行原理"></a>执行原理</h3><ul><li>均支持三种检测探针：TCP, HTTP, Exec Shell</li><li>探针的执行均由kubelet组件执行；</li></ul><h4 id="Exec探针执行"><a href="#Exec探针执行" class="headerlink" title="Exec探针执行"></a>Exec探针执行</h4><ul><li>由kubelet组件调用CRI接口的ExecSync接口，在对应的容器内执对应的cmd命令，获取其返回值；</li></ul><span id="more"></span><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(pb *prober)</span></span> runProbe(p *v1.Probe, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID) (probe.Result, <span class="type">string</span>, <span class="type">error</span>) &#123;</span><br><span class="line">    ...     </span><br><span class="line">    command := kubecontainer.ExpandContainerCommandOnlyStatic(p.Exec.Command, container.Env)</span><br><span class="line">    <span class="keyword">return</span> pb.exec.Probe(pb.newExecInContainer(container, containerID, command, timeout))</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(pb *prober)</span></span> newExecInContainer(container v1.Container, containerID kubecontainer.ContainerID, cmd []<span class="type">string</span>, timeout time.Duration) exec.Cmd &#123;</span><br><span class="line"><span class="keyword">return</span> execInContainer&#123;<span class="function"><span class="keyword">func</span><span class="params">()</span></span> ([]<span class="type">byte</span>, <span class="type">error</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> pb.runner.RunInContainer(containerID, cmd, timeout)</span><br><span class="line">&#125;&#125;</span><br><span class="line">&#125;</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *kubeGenericRuntimeManager)</span></span> RunInContainer(id kubecontainer.ContainerID, cmd []<span class="type">string</span>, timeout time.Duration) ([]<span class="type">byte</span>, <span class="type">error</span>) &#123;</span><br><span class="line">stdout, stderr, err := m.runtimeService.ExecSync(id.ID, cmd, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">append</span>(stdout, stderr...), err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(pr execProber)</span></span> Probe(e exec.Cmd) (probe.Result, <span class="type">string</span>, <span class="type">error</span>) &#123;</span><br><span class="line">data, err := e.CombinedOutput()</span><br><span class="line">glog.V(<span class="number">4</span>).Infof(<span class="string">&quot;Exec probe response: %q&quot;</span>, <span class="type">string</span>(data))</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">exit, ok := err.(exec.ExitError)</span><br><span class="line"><span class="keyword">if</span> ok &#123;</span><br><span class="line"><span class="keyword">if</span> exit.ExitStatus() == <span class="number">0</span> &#123;</span><br><span class="line"><span class="keyword">return</span> probe.Success, <span class="type">string</span>(data), <span class="literal">nil</span></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">return</span> probe.Failure, <span class="type">string</span>(data), <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> probe.Unknown, <span class="string">&quot;&quot;</span>, err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> probe.Success, <span class="type">string</span>(data), <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="HTTP探针执行"><a href="#HTTP探针执行" class="headerlink" title="HTTP探针执行"></a>HTTP探针执行</h4><ul><li>由kubelet请求容器指定的URL，根据response status来判断，status属于<code>[200-400)</code>即判断成功：<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">DoHTTPProbe</span><span class="params">(url *url.URL, headers http.Header, client HTTPGetInterface)</span></span> (probe.Result, <span class="type">string</span>, <span class="type">error</span>) &#123;</span><br><span class="line">req, err := http.NewRequest(<span class="string">&quot;GET&quot;</span>, url.String(), <span class="literal">nil</span>)</span><br><span class="line">......</span><br><span class="line">    <span class="keyword">if</span> res.StatusCode &gt;= http.StatusOK &amp;&amp; res.StatusCode &lt; http.StatusBadRequest &#123;</span><br><span class="line">glog.V(<span class="number">4</span>).Infof(<span class="string">&quot;Probe succeeded for %s, Response: %v&quot;</span>, url.String(), *res)</span><br><span class="line"><span class="keyword">return</span> probe.Success, body, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line">......</span><br></pre></td></tr></table></figure></li></ul><h4 id="TCP探针执行"><a href="#TCP探针执行" class="headerlink" title="TCP探针执行"></a>TCP探针执行</h4><ul><li>由kubelet执行<code>Dail</code>操作判断端口是否可连接：<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">DoTCPProbe</span><span class="params">(addr <span class="type">string</span>, timeout time.Duration)</span></span> (probe.Result, <span class="type">string</span>, <span class="type">error</span>) &#123;</span><br><span class="line">conn, err := net.DialTimeout(<span class="string">&quot;tcp&quot;</span>, addr, timeout)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="comment">// Convert errors to failures to handle timeouts.</span></span><br><span class="line"><span class="keyword">return</span> probe.Failure, err.Error(), <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line">err = conn.Close()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">glog.Errorf(<span class="string">&quot;Unexpected error closing TCP probe socket: %v (%#v)&quot;</span>, err, err)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> probe.Success, <span class="string">&quot;&quot;</span>, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="StartupProbe作用"><a href="#StartupProbe作用" class="headerlink" title="StartupProbe作用"></a>StartupProbe作用</h3><p><img src="/2024/02/20/k8s/probe/startupProbe.png" alt="startupProbe"></p><ul><li>对于一些服务场景，服务启动时间较长，以往的做法是调大<code>LivenessProbe</code>的<code>initialDelaySeconds</code>和<code>failureThreshold</code>值，但如果超过这个时间范围还没启动，服务会进入重启，并进入循环卡死的过程；</li><li>因此，<code>StartupProbe</code>就是用于解决这种场景下的问题，在<code>StartupProbe</code>成功之前，是不是为执行<code>LivenessProbe</code>和<code>ReadinessProbe</code>的；</li></ul><h3 id="LivenessProbe作用"><a href="#LivenessProbe作用" class="headerlink" title="LivenessProbe作用"></a>LivenessProbe作用</h3><ul><li>可以恢复 运行过程pod出现卡死的情况；</li><li>如果pod的该探针失败，则k8s会触发该pod的删除与重建流程；</li></ul><h3 id="ReadinessProbe作用"><a href="#ReadinessProbe作用" class="headerlink" title="ReadinessProbe作用"></a>ReadinessProbe作用</h3><ul><li><p>影响pod的流量开关；</p></li><li><p>负责判断pod是否就绪，只有就绪状态后，才会将pod IP放到service的endpoints上，并开始接收流量；</p></li><li><p>kubelet会通过<code>SetContainerReadiness</code>将container的condition设置为true：</p><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *manager)</span></span> SetContainerReadiness(podUID types.UID, containerID kubecontainer.ContainerID, ready <span class="type">bool</span>) &#123;</span><br><span class="line">    ......</span><br><span class="line">    containerStatus.Ready = ready</span><br><span class="line">        ......</span><br><span class="line">    readyCondition := GeneratePodReadyCondition(&amp;pod.Spec, status.ContainerStatuses, status.Phase)</span><br><span class="line">    ......</span><br><span class="line">    m.updateStatusInternal(pod, status, <span class="literal">false</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>k8s v1.12之前，pod是否处于ready是由kubelet根据容器状态来判断的，如果pod中容器全部处于ready，则pod处于ready状态；</p></li><li><p>k8s v1.12之后，提供了一个<code>readinessGates</code>的功能来满足用于对于pod ready状态的控制需求，此时判断pod是否ready有两个前提条件：</p><ul><li>Pod中所有容器全部ready，即<code>ContainersReady</code>为<code>True</code>；</li><li><code>pod.spec.readinessGates</code>中定义一个或多个<code>conditionType</code>，需要这些<code>conditionType</code>都为<code>True</code>，pod才能为ready；<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">readinessGates:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">conditionType:</span> <span class="string">MyDemo</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">conditions:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">MyDemo</span></span><br><span class="line">    <span class="attr">status:</span> <span class="string">&quot;True&quot;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">ContainersReady</span></span><br><span class="line">    <span class="attr">status:</span> <span class="string">&quot;True&quot;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">Ready</span></span><br><span class="line">    <span class="attr">status:</span> <span class="string">&quot;True&quot;</span></span><br></pre></td></tr></table></figure></li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Readiness-Liveness-StartupProbe&quot;&gt;&lt;a href=&quot;#Readiness-Liveness-StartupProbe&quot; class=&quot;headerlink&quot; title=&quot;Readiness, Liveness, StartupProbe&quot;&gt;&lt;/a&gt;Readiness, Liveness, StartupProbe&lt;/h2&gt;&lt;h3 id=&quot;执行原理&quot;&gt;&lt;a href=&quot;#执行原理&quot; class=&quot;headerlink&quot; title=&quot;执行原理&quot;&gt;&lt;/a&gt;执行原理&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;均支持三种检测探针：TCP, HTTP, Exec Shell&lt;/li&gt;
&lt;li&gt;探针的执行均由kubelet组件执行；&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;Exec探针执行&quot;&gt;&lt;a href=&quot;#Exec探针执行&quot; class=&quot;headerlink&quot; title=&quot;Exec探针执行&quot;&gt;&lt;/a&gt;Exec探针执行&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;由kubelet组件调用CRI接口的ExecSync接口，在对应的容器内执对应的cmd命令，获取其返回值；&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="k8s" scheme="https://so2bin.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s client-go</title>
    <link href="https://so2bin.github.io/2024/02/19/k8s/client-go/"/>
    <id>https://so2bin.github.io/2024/02/19/k8s/client-go/</id>
    <published>2024-02-19T14:01:07.000Z</published>
    <updated>2024-03-01T16:09:42.927Z</updated>
    
    <content type="html"><![CDATA[<h2 id="client"><a href="#client" class="headerlink" title="client"></a>client</h2><ul><li>k8s client-go提供4个client：<code>RESTClient</code>, <code>ClientSet</code>, <code>DynamicClient</code>, <code>DiscoveryClient</code></li></ul><h3 id="RESTClient"><a href="#RESTClient" class="headerlink" title="RESTClient"></a>RESTClient</h3><ul><li>作为另外三类client的基类，提供与k8s apiserver的底层API请求构造与请求；</li></ul><h3 id="ClientSet"><a href="#ClientSet" class="headerlink" title="ClientSet"></a>ClientSet</h3><ul><li>用于方便访问k8s内置api group资源的client，如deployment, pod等；</li></ul><h3 id="DynamicClient"><a href="#DynamicClient" class="headerlink" title="DynamicClient"></a>DynamicClient</h3><ul><li>通过指定资源组，资源版本即可操作k8s中的任务资源，包括CRD和内置资源；</li><li>DynamicClient使用嵌套的<code>map[string]interface&#123;&#125;</code>结构来存储k8s apiserver的返回资源结构，通过反射机制，在运行时进行数据绑定；</li></ul><h3 id="DiscoveryClient"><a href="#DiscoveryClient" class="headerlink" title="DiscoveryClient"></a>DiscoveryClient</h3><ul><li>与前面三种client不同，该client不是用于管理k8s资源对象的，该client是用于查询k8s支持的GVR资源类型的，与<code>kubectl api-versions</code>和<code>kubectl api-resources</code>返回的资源内容相关；</li></ul><span id="more"></span><h2 id="Informer"><a href="#Informer" class="headerlink" title="Informer"></a>Informer</h2><h3 id="Reflector-and-List-Watch"><a href="#Reflector-and-List-Watch" class="headerlink" title="Reflector and List-Watch"></a>Reflector and List-Watch</h3><ul><li><p>资料：<a href="https://xiaorui.cc/archives/7361">https://xiaorui.cc/archives/7361</a></p></li><li><p>List: 通过k8s apiserver Restful API获取全量数据列表，并同步到本地缓存中，该操作基于HTTP短连接实现；</p></li><li><p>Watch负责监听资源变化，并调用相应事件处理函数进行处理，如更新本地缓存，使本地缓存与ETCD保持一致，该操作基于HTTP长连接实现；</p></li><li><p>Reflector是client-go中用于监听指定资源的组件，当资源发生变化时，会以事件的形式存储到本地队列，然后触发后续的相应处理函数；</p></li><li><p>Reflector核心逻辑有三部分：</p><ul><li>List: 全量同步指定资源；</li><li>Watch: 监听资源的变更；</li><li>定时同步：定时更触发同步机制，定时更新缓存数据，可配置定时同步的周期；</li></ul></li><li><p>当Watch断开时，Reflector会重新走List-Watch流程；</p></li><li><p>第一次执行List操作时，由于<code>resourceVersion</code>为空，拉取的是全是数据；</p></li><li><p>当list-wathc出现异常重试时，List会根据本地存储的最新的<code>resourceVersion</code>拉取其之后的所有新数据；</p></li></ul><h3 id="DeltaFIFO"><a href="#DeltaFIFO" class="headerlink" title="DeltaFIFO"></a>DeltaFIFO</h3><ul><li>增量的本地阶段，记录了资源的变化过程；</li><li>FIFO即为先入先出的本地队列；</li><li>Delta为资源的变化事件，存储的数据结构<code>Deltas</code>有两个属性：<code>Type</code>, <code>Object</code>，分别表示事件类型（增删改），资源类型；</li><li>FIFO负责接收来自Reflector的事件，将按顺序存储，多个相同事件只会被处理一次；</li></ul><h3 id="Indexer"><a href="#Indexer" class="headerlink" title="Indexer"></a>Indexer</h3><ul><li>本身是一个本地存储，并扩展了一定的索引能力；</li><li>Reflector通过DeltaFIFO的操作后，数据会被存储到Indexer中；</li><li>Indexer中的数据与ETCD完全一致；</li></ul><p><strong>几个重要概念</strong></p><ul><li><code>IndexFunc</code>: 索引函数，用于计算一个资源对象的索引值列表，可以根据需要自定义，如通过label, annotation来生成索引列表；</li><li><code>Index</code>：存储数据，如要查找某个ns下的pod，就要让pod按ns空间进行索引，对应的Index类型即为<code>map[ns]pods</code>；</li><li><code>Indexers</code>：存储索引器，key为索引器名称，value为索引器实现函数（IndexFunc），如<code>map[ns]XXXIndexFunc</code><blockquote><p>即存储索引的存储器</p></blockquote></li><li><code>Indices</code>：存储缓存器，key为索引器名称，value为缓存的数据（Index），如<code>map[ns]map[ns]sets.pod</code><blockquote><p>即存储缓存数据的存储器</p></blockquote></li></ul><figure class="highlight go"><table><tr><td class="code"><pre><span class="line">index := cache.NewIndexer(</span><br><span class="line">    cache.MetaNamespaceKeyFunc,  <span class="comment">// 资源key生成函数</span></span><br><span class="line">    cache.Indexers&#123;  <span class="comment">// 索引存储器名 =&gt; 索引生成函数</span></span><br><span class="line">        <span class="string">&quot;namespace&quot;</span>: XXXNsIndexFunc, </span><br><span class="line">        <span class="string">&quot;nodeName&quot;</span>: XXNodeNameIndexFunc,</span><br><span class="line">    &#125;)</span><br><span class="line">pod1 := &amp;v1.Pod&#123;...&#125;</span><br><span class="line">pod2 := &amp;v1.Pod&#123;...&#125;</span><br><span class="line">pod3 := &amp;v1.Pod&#123;...&#125;</span><br><span class="line"></span><br><span class="line">_ = index.Add(pod1)</span><br><span class="line">_ = index.Add(pod2)</span><br><span class="line">_ = index.Add(pod3)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查询</span></span><br><span class="line">pods, err := index.ByIndex(<span class="string">&quot;namespace&quot;</span>, <span class="string">&quot;xxx&quot;</span>)</span><br><span class="line">pods, err = index.ByIndex(<span class="string">&quot;nodeName&quot;</span>, <span class="string">&quot;xxx&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="SharedInformer"><a href="#SharedInformer" class="headerlink" title="SharedInformer"></a>SharedInformer</h2><ul><li>通过<code>SharedInformer</code>为多个Controller提供缓存资源对象的informer共享，避免重复缓存；</li><li><code>SharedInformer</code>一般是使用<code>SharedInformerFactory</code>来管理控制器需要的资源对象的informer，使用map结构来存储资源对象的informer；</li></ul><h2 id="SharedIndexInformer"><a href="#SharedIndexInformer" class="headerlink" title="SharedIndexInformer"></a>SharedIndexInformer</h2><ul><li>继承了<code>SharedInformer</code>，并扩展了添加和获取Indexers的能力；</li></ul><h2 id="SharedInformerFactory"><a href="#SharedInformerFactory" class="headerlink" title="SharedInformerFactory"></a>SharedInformerFactory</h2><ul><li><code>SharedInformerFactory</code>为所有已知GVR提供共享的informer；</li><li>其有一个<code>Start</code>方法，用于启动所有Reflector的Informer；</li><li>其<code>WaitForCacheSync</code>函数会不断调用factory持有的informer的<code>HasSynced</code>方案直接全部返回true，表示所有informer关注的资源对象已经全部缓存到了本地；</li><li><code>InformerFor</code>方法，是用于返回<code>SharedInformerFactory</code>中指定资源类型所对应的informer，如果不存在则会先创建；</li><li>k8s client-go已经提供了内置的常用informer，如<code>PodInforer</code>, <code>ServiceInformer</code>等；</li></ul><figure class="highlight go"><table><tr><td class="code"><pre><span class="line">sharedInformerFactory := informers.NewSharedInformerFactory(clientSet, <span class="number">0</span>)</span><br><span class="line">podInformer := sharedInformerFactory.Core().V1().Pods()</span><br><span class="line"><span class="comment">// 生成一个indexer便于查询数据</span></span><br><span class="line">indexer := podInformer.Lister()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 启动informer</span></span><br><span class="line">sharedInformerFactory.Start(<span class="literal">nil</span>)</span><br><span class="line">sharedInformerFactory.WaitForCacheSync(<span class="literal">nil</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查询pod</span></span><br><span class="line">pods, err := indexer.List(labels.Everything())</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;client&quot;&gt;&lt;a href=&quot;#client&quot; class=&quot;headerlink&quot; title=&quot;client&quot;&gt;&lt;/a&gt;client&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;k8s client-go提供4个client：&lt;code&gt;RESTClient&lt;/code&gt;, &lt;code&gt;ClientSet&lt;/code&gt;, &lt;code&gt;DynamicClient&lt;/code&gt;, &lt;code&gt;DiscoveryClient&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;RESTClient&quot;&gt;&lt;a href=&quot;#RESTClient&quot; class=&quot;headerlink&quot; title=&quot;RESTClient&quot;&gt;&lt;/a&gt;RESTClient&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;作为另外三类client的基类，提供与k8s apiserver的底层API请求构造与请求；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;ClientSet&quot;&gt;&lt;a href=&quot;#ClientSet&quot; class=&quot;headerlink&quot; title=&quot;ClientSet&quot;&gt;&lt;/a&gt;ClientSet&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;用于方便访问k8s内置api group资源的client，如deployment, pod等；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;DynamicClient&quot;&gt;&lt;a href=&quot;#DynamicClient&quot; class=&quot;headerlink&quot; title=&quot;DynamicClient&quot;&gt;&lt;/a&gt;DynamicClient&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;通过指定资源组，资源版本即可操作k8s中的任务资源，包括CRD和内置资源；&lt;/li&gt;
&lt;li&gt;DynamicClient使用嵌套的&lt;code&gt;map[string]interface&amp;#123;&amp;#125;&lt;/code&gt;结构来存储k8s apiserver的返回资源结构，通过反射机制，在运行时进行数据绑定；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;DiscoveryClient&quot;&gt;&lt;a href=&quot;#DiscoveryClient&quot; class=&quot;headerlink&quot; title=&quot;DiscoveryClient&quot;&gt;&lt;/a&gt;DiscoveryClient&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;与前面三种client不同，该client不是用于管理k8s资源对象的，该client是用于查询k8s支持的GVR资源类型的，与&lt;code&gt;kubectl api-versions&lt;/code&gt;和&lt;code&gt;kubectl api-resources&lt;/code&gt;返回的资源内容相关；&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="k8s" scheme="https://so2bin.github.io/tags/k8s/"/>
    
    <category term="client-go" scheme="https://so2bin.github.io/tags/client-go/"/>
    
  </entry>
  
  <entry>
    <title>raft</title>
    <link href="https://so2bin.github.io/2024/02/19/raft/"/>
    <id>https://so2bin.github.io/2024/02/19/raft/</id>
    <published>2024-02-19T12:36:37.000Z</published>
    <updated>2024-02-21T11:02:07.691Z</updated>
    
    <content type="html"><![CDATA[<h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><ul><li><a href="https://raft.github.io/">https://raft.github.io/</a></li></ul><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><ol><li><p>随机超时：避免同时发起选举请求</p><img src="/2024/02/19/raft/raft-random-timeout.png" width="30%" alt="raft random timeout"></li><li><p>任期Term：用于标记选举任期，可用于让故障回复后的leader变成follower，任期小的选举请求会被拒绝；</p></li><li><p>角色：leader, follower, candidate</p></li><li><p>leader选择出来后，leader会定时向follower发起心跳，如心跳超时，则最先timeout的节点会发起选举请求，此时该节点会从follower转变成cadidate；</p></li></ol><span id="more"></span>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;资料&quot;&gt;&lt;a href=&quot;#资料&quot; class=&quot;headerlink&quot; title=&quot;资料&quot;&gt;&lt;/a&gt;资料&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://raft.github.io/&quot;&gt;https://raft.github.io/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;特点&quot;&gt;&lt;a href=&quot;#特点&quot; class=&quot;headerlink&quot; title=&quot;特点&quot;&gt;&lt;/a&gt;特点&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;随机超时：避免同时发起选举请求&lt;/p&gt;
&lt;img src=&quot;/2024/02/19/raft/raft-random-timeout.png&quot; width=&quot;30%&quot; alt=&quot;raft random timeout&quot;&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;任期Term：用于标记选举任期，可用于让故障回复后的leader变成follower，任期小的选举请求会被拒绝；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;角色：leader, follower, candidate&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;leader选择出来后，leader会定时向follower发起心跳，如心跳超时，则最先timeout的节点会发起选举请求，此时该节点会从follower转变成cadidate；&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    
    <category term="raft" scheme="https://so2bin.github.io/tags/raft/"/>
    
  </entry>
  
  <entry>
    <title>cni</title>
    <link href="https://so2bin.github.io/2024/02/18/cloud-native/cni/"/>
    <id>https://so2bin.github.io/2024/02/18/cloud-native/cni/</id>
    <published>2024-02-18T14:15:43.000Z</published>
    <updated>2024-02-18T16:11:48.137Z</updated>
    
    <content type="html"><![CDATA[<h2 id="容器网络"><a href="#容器网络" class="headerlink" title="容器网络"></a>容器网络</h2><blockquote><ul><li><a href="https://zhuanlan.zhihu.com/p/364886965">https://zhuanlan.zhihu.com/p/364886965</a></li></ul></blockquote><h3 id="docker网络模式"><a href="#docker网络模式" class="headerlink" title="docker网络模式"></a>docker网络模式</h3><ul><li>默认创建如下三种：bridge, host, none<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbb@hbb:~$ docker network <span class="built_in">ls</span></span><br><span class="line">NETWORK ID     NAME      DRIVER    SCOPE</span><br><span class="line">3c92bea4cc95   bridge    bridge    <span class="built_in">local</span></span><br><span class="line">3d964312d0b5   host      host      <span class="built_in">local</span></span><br><span class="line">07d5e640e74f   none      null      <span class="built_in">local</span></span><br></pre></td></tr></table></figure></li><li>默认容器均采用bridge网桥模式，所有容器均会创建一对虚拟网络接口，容器内一个eth0，容器外一个vethXXX，vethXXX属于docker0，通过docker0实现对外网的访问；</li><li><code>host</code>模式即与主机共享网络空间，不使用私有的容器网络空间；</li><li><code>none</code>模式即容器内没有eth0网口，因此无法与容器外通信；</li></ul><h3 id="network-namespace-veth-pair-bridge"><a href="#network-namespace-veth-pair-bridge" class="headerlink" title="network namespace, veth pair, bridge"></a>network namespace, veth pair, bridge</h3><ul><li>下图所示为相关概念：<br><img src="/2024/02/18/cloud-native/cni/basic-concepts.png" alt="network namespace vs veth pair vs bridge"></li></ul><h4 id="bridge"><a href="#bridge" class="headerlink" title="bridge"></a><code>bridge</code></h4><ul><li>docker0 bridge即纯软件实现的虚拟交换机，可实现与物理交换机同样的功能，如二层交换，ARP寻找MAC地址；</li></ul><h4 id="network-namespace"><a href="#network-namespace" class="headerlink" title="network namespace"></a><code>network namespace</code></h4><ul><li>即将一个物理的网络协议栈进行逻辑隔离，形成网络空间，不同的拷贝协议栈有自己独立的网络接口，ip，route tables, iptables等，而进程可以设置使用不同的network namespace，多个进程也可以属于同一个ns，一个pod有一个独立的ns，pod内的容器属于同一个ns；</li></ul><h3 id="容器网络通信模型"><a href="#容器网络通信模型" class="headerlink" title="容器网络通信模型"></a>容器网络通信模型</h3><ul><li>pod内容器属于同一个网络空间，所以可直接相互访问；</li><li>同机不同容器之间，通过虚拟以太网veth network互联，docker安装后，会默认创建一个docker0作为虚拟以太网的bridge网桥，在容器创建时会在容器内外创建一对虚拟网络接口，容器内为eth0，容器外即主机上会对应创建一个vethXXX名的接口名，如下图所示：<ul><li>该图为docker容器内eth0，与docker0属于同一网段：<br><img src="/2024/02/18/cloud-native/cni/vethInC.png" alt="veth in C"></li><li>该图为上面容器在主机上对应的veth网口，其master为docker0：<br><img src="/2024/02/18/cloud-native/cni/vethOnHost.png" alt="veth on host"></li></ul></li><li>在主机上通过<code>docker inspect network bridge</code>命令可查看docker列出的当前docker默认bridge网桥docker0下的容器网络信息：<br><img src="/2024/02/18/cloud-native/cni/docker-bridge.png" alt="docker bridge"></li></ul><span id="more"></span><ul><li>因此，同主机上的容器之间是通过网桥来实现互联，其为2层网络通信；</li><li>k8s跨节点的网络通信，需要对应的CNI插件来创建和维护，其原理是通过IP包路由来实现；</li><li>因为k8s的容器编排会导致pod频繁变化，对应的IP也会频繁变化，因此实现更新对应的网络信息是需要对应的CNI插件来实现，不同插件实现的跨节点的网络通信方式不一样，如通过TUN隧道等；<br><img src="/2024/02/18/cloud-native/cni/k8s-network.png" alt="k8s network"></li></ul><h3 id="pod网络创建过程"><a href="#pod网络创建过程" class="headerlink" title="pod网络创建过程"></a>pod网络创建过程</h3><ul><li>kubelet -&gt; CRI plugin -&gt; pod net ns, pause sandox -&gt; CNI plugin</li><li>如下图所示：<br><img src="/2024/02/18/cloud-native/cni/pod-network.png" alt="pod network"></li></ul><h3 id="vxlan网络与flannel-CNI"><a href="#vxlan网络与flannel-CNI" class="headerlink" title="vxlan网络与flannel CNI"></a>vxlan网络与flannel CNI</h3><ul><li><p>如图所示，为vxlan原理：<br><img src="/2024/02/18/cloud-native/cni/vxlan.png" alt="vxlan"></p></li><li><p>flannel CNI插件会为每个node分配一个子网IP段，上面的pod会从该网段范围取一个IP；</p></li><li><p>默认，flannel跨节点通信是通过<code>vxlan</code>实现，即通过隧道技术<code>L2 Oery UDP</code>；</p></li></ul><h3 id="calico-CNI"><a href="#calico-CNI" class="headerlink" title="calico CNI"></a>calico CNI</h3><ul><li><p>calico是一个运行在纯三层的组件，基于BGP协议和Linux自身的路由转发机制，容器通信也不依赖iptables NAT或Tunnel 等技术，因此相对于flannel会少一些封包与拆包的过程，效率更高：<br><img src="/2024/02/18/cloud-native/cni/calico.png" alt="calico"></p></li><li><p>其跨主机的通信模型如下所示：<br><img src="/2024/02/18/cloud-native/cni/calico-communicate.png" alt="calico communicate"></p></li><li><p>当容器创建时，calico为容器生成veth pair，一端作为容器网卡加入到容器的网络命名空间，并设置IP和掩码，一端直接暴露在宿主机上，并通过设置路由规则，将容器IP暴露到宿主机的通信路由上。于此同时，calico为每个主机分配了一段子网作为容器可分配的IP范围，这样就可以根据子网的CIDR为每个主机生成比较固定的路由规则。</p></li><li><p>当容器需要跨主机通信时，主要经过下面的简单步骤：</p><ul><li>容器流量通过veth pair到达宿主机的网络命名空间上。</li><li>根据容器要访问的IP所在的子网CIDR和主机上的路由规则，找到下一跳要到达的宿主机IP。</li><li>流量到达下一跳的宿主机后，根据当前宿主机上的路由规则，直接到达对端容器的veth pair插在宿主机的一端，最终进入容器。</li></ul></li></ul><h2 id="集群网络"><a href="#集群网络" class="headerlink" title="集群网络"></a>集群网络</h2><ul><li>通过service来维护pod IP的抽象，管理与负载均衡；</li><li>通过环境变量或DNS来实现服务发现；</li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;容器网络&quot;&gt;&lt;a href=&quot;#容器网络&quot; class=&quot;headerlink&quot; title=&quot;容器网络&quot;&gt;&lt;/a&gt;容器网络&lt;/h2&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/364886965&quot;&gt;https://zhuanlan.zhihu.com/p/364886965&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;docker网络模式&quot;&gt;&lt;a href=&quot;#docker网络模式&quot; class=&quot;headerlink&quot; title=&quot;docker网络模式&quot;&gt;&lt;/a&gt;docker网络模式&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;默认创建如下三种：bridge, host, none&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;hbb@hbb:~$ docker network &lt;span class=&quot;built_in&quot;&gt;ls&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;NETWORK ID     NAME      DRIVER    SCOPE&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3c92bea4cc95   bridge    bridge    &lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3d964312d0b5   host      host      &lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;07d5e640e74f   none      null      &lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;li&gt;默认容器均采用bridge网桥模式，所有容器均会创建一对虚拟网络接口，容器内一个eth0，容器外一个vethXXX，vethXXX属于docker0，通过docker0实现对外网的访问；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;host&lt;/code&gt;模式即与主机共享网络空间，不使用私有的容器网络空间；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;none&lt;/code&gt;模式即容器内没有eth0网口，因此无法与容器外通信；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;network-namespace-veth-pair-bridge&quot;&gt;&lt;a href=&quot;#network-namespace-veth-pair-bridge&quot; class=&quot;headerlink&quot; title=&quot;network namespace, veth pair, bridge&quot;&gt;&lt;/a&gt;network namespace, veth pair, bridge&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;下图所示为相关概念：&lt;br&gt;&lt;img src=&quot;/2024/02/18/cloud-native/cni/basic-concepts.png&quot; alt=&quot;network namespace vs veth pair vs bridge&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;bridge&quot;&gt;&lt;a href=&quot;#bridge&quot; class=&quot;headerlink&quot; title=&quot;bridge&quot;&gt;&lt;/a&gt;&lt;code&gt;bridge&lt;/code&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;docker0 bridge即纯软件实现的虚拟交换机，可实现与物理交换机同样的功能，如二层交换，ARP寻找MAC地址；&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;network-namespace&quot;&gt;&lt;a href=&quot;#network-namespace&quot; class=&quot;headerlink&quot; title=&quot;network namespace&quot;&gt;&lt;/a&gt;&lt;code&gt;network namespace&lt;/code&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;即将一个物理的网络协议栈进行逻辑隔离，形成网络空间，不同的拷贝协议栈有自己独立的网络接口，ip，route tables, iptables等，而进程可以设置使用不同的network namespace，多个进程也可以属于同一个ns，一个pod有一个独立的ns，pod内的容器属于同一个ns；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;容器网络通信模型&quot;&gt;&lt;a href=&quot;#容器网络通信模型&quot; class=&quot;headerlink&quot; title=&quot;容器网络通信模型&quot;&gt;&lt;/a&gt;容器网络通信模型&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;pod内容器属于同一个网络空间，所以可直接相互访问；&lt;/li&gt;
&lt;li&gt;同机不同容器之间，通过虚拟以太网veth network互联，docker安装后，会默认创建一个docker0作为虚拟以太网的bridge网桥，在容器创建时会在容器内外创建一对虚拟网络接口，容器内为eth0，容器外即主机上会对应创建一个vethXXX名的接口名，如下图所示：&lt;ul&gt;
&lt;li&gt;该图为docker容器内eth0，与docker0属于同一网段：&lt;br&gt;&lt;img src=&quot;/2024/02/18/cloud-native/cni/vethInC.png&quot; alt=&quot;veth in C&quot;&gt;&lt;/li&gt;
&lt;li&gt;该图为上面容器在主机上对应的veth网口，其master为docker0：&lt;br&gt;&lt;img src=&quot;/2024/02/18/cloud-native/cni/vethOnHost.png&quot; alt=&quot;veth on host&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;在主机上通过&lt;code&gt;docker inspect network bridge&lt;/code&gt;命令可查看docker列出的当前docker默认bridge网桥docker0下的容器网络信息：&lt;br&gt;&lt;img src=&quot;/2024/02/18/cloud-native/cni/docker-bridge.png&quot; alt=&quot;docker bridge&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="k8s" scheme="https://so2bin.github.io/tags/k8s/"/>
    
    <category term="CNI" scheme="https://so2bin.github.io/tags/CNI/"/>
    
  </entry>
  
  <entry>
    <title>Go Channel</title>
    <link href="https://so2bin.github.io/2024/01/28/Go/channel/"/>
    <id>https://so2bin.github.io/2024/01/28/Go/channel/</id>
    <published>2024-01-28T12:01:26.000Z</published>
    <updated>2024-01-28T12:15:58.832Z</updated>
    
    <content type="html"><![CDATA[<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><ul><li>本质上，channel是一个包含：发送Go队列，接收Go队列，由循环数组实现的缓存数据队列，锁这几个元素组成的结构体实现；</li><li><code>sendmsg</code>阶段：<ul><li>如果接收Go队列有值，则直接进行值元素的内存拷贝，并唤醒接收队列；</li><li>如有缓冲队列且未满，则将值复制到循环队列中；</li><li>如果有缓冲队列且满了或无缓冲队列，将协程放到channel的<code>sendqueue</code>中；</li></ul></li><li><code>recvmsg</code>阶段：<ul><li>如果缓冲，且无数据，则进入<code>recvqueue</code>中，协程挂起(<code>gopack</code>)；</li><li>如果缓冲队列中数据，由从队列中将值拷贝给当前接收者；</li><li>如果<code>sendqueue</code>有协程，则说明缓冲队列已满，则从缓冲队列中取值，并将<code>sendqueue</code>中取一个协程，将其值拷贝到缓冲循环数组中；</li></ul></li><li>上述操作是在mutex锁下进行，所以channel是协程安全的；</li></ul><span id="more"></span>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;原理&quot;&gt;&lt;a href=&quot;#原理&quot; class=&quot;headerlink&quot; title=&quot;原理&quot;&gt;&lt;/a&gt;原理&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;本质上，channel是一个包含：发送Go队列，接收Go队列，由循环数组实现的缓存数据队列，锁这几个元素组成的结构体实现；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sendmsg&lt;/code&gt;阶段：&lt;ul&gt;
&lt;li&gt;如果接收Go队列有值，则直接进行值元素的内存拷贝，并唤醒接收队列；&lt;/li&gt;
&lt;li&gt;如有缓冲队列且未满，则将值复制到循环队列中；&lt;/li&gt;
&lt;li&gt;如果有缓冲队列且满了或无缓冲队列，将协程放到channel的&lt;code&gt;sendqueue&lt;/code&gt;中；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;recvmsg&lt;/code&gt;阶段：&lt;ul&gt;
&lt;li&gt;如果缓冲，且无数据，则进入&lt;code&gt;recvqueue&lt;/code&gt;中，协程挂起(&lt;code&gt;gopack&lt;/code&gt;)；&lt;/li&gt;
&lt;li&gt;如果缓冲队列中数据，由从队列中将值拷贝给当前接收者；&lt;/li&gt;
&lt;li&gt;如果&lt;code&gt;sendqueue&lt;/code&gt;有协程，则说明缓冲队列已满，则从缓冲队列中取值，并将&lt;code&gt;sendqueue&lt;/code&gt;中取一个协程，将其值拷贝到缓冲循环数组中；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;上述操作是在mutex锁下进行，所以channel是协程安全的；&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="Go" scheme="https://so2bin.github.io/tags/Go/"/>
    
    <category term="Channel" scheme="https://so2bin.github.io/tags/Channel/"/>
    
  </entry>
  
  <entry>
    <title>Go GMP and Scheduler</title>
    <link href="https://so2bin.github.io/2024/01/28/Go/GMP%20and%20Scheduler/"/>
    <id>https://so2bin.github.io/2024/01/28/Go/GMP%20and%20Scheduler/</id>
    <published>2024-01-28T11:58:17.000Z</published>
    <updated>2024-02-04T08:52:51.069Z</updated>
    
    <content type="html"><![CDATA[<h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><ul><li><a href="https://medium.com/@sanilkhurana7/understanding-the-go-scheduler-and-looking-at">https://medium.com/@sanilkhurana7/understanding-the-go-scheduler-and-looking-at</a>   1-how-it-works-e431a6daacf</li><li><a href="https://segmentfault.com/a/1190000041860912/en">https://segmentfault.com/a/1190000041860912/en</a></li></ul><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><ul><li>G：协程，P：processor，M：系统线程，M绑定一个P才能执行G；</li><li>P的数据是控制着Go的并发能力，由环境变量<code>GOMAXPROCESS</code>控制数量，线程数一般不配置，但也有一个底层的函数可设置（<code>SetMaxThreads</code>），M默认上限是10000；</li><li>P的存在是为了实现G的调度和M&#x2F;G之间M:N的关系；</li><li>P有一个local queue，无锁，当P的local queue无G时，会从Global queue取，此时需要用锁，而如果Global queue也无G时，会走work stealing，尝试从其它P的local queue上找G运行；</li><li>G的抢占调度是基于时间片，即如果G长时间连续运行超过10ms，则会通过系统信号SIGURG强制调度该协程；</li><li>当G发生阻塞，如系统调用IO操作时，会创建一个新的线程或利用一个空闲的线程来绑定当前P，以继续执行P上的G，而旧线程进入阻塞睡眠状态；</li><li>P的local queue放的G的数量上限为256；</li><li>当因IO阻塞进入idle状态的线程，在G系统调用结束时，因此时M上没有P了，而G的执行一定需要P，所以会尝试先去获取一个idle的P来执行，如找到了，则会将该G放到该P的local queue中，如果没有，则M会进入idle线程队列，G会放到Global queue中；</li></ul><span id="more"></span>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;资料&quot;&gt;&lt;a href=&quot;#资料&quot; class=&quot;headerlink&quot; title=&quot;资料&quot;&gt;&lt;/a&gt;资料&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://medium.com/@sanilkhurana7/understanding-the-go-scheduler-and-looking-at&quot;&gt;https://medium.com/@sanilkhurana7/understanding-the-go-scheduler-and-looking-at&lt;/a&gt;   1-how-it-works-e431a6daacf&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://segmentfault.com/a/1190000041860912/en&quot;&gt;https://segmentfault.com/a/1190000041860912/en&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;结构&quot;&gt;&lt;a href=&quot;#结构&quot; class=&quot;headerlink&quot; title=&quot;结构&quot;&gt;&lt;/a&gt;结构&lt;/h2&gt;&lt;h2 id=&quot;原理&quot;&gt;&lt;a href=&quot;#原理&quot; class=&quot;headerlink&quot; title=&quot;原理&quot;&gt;&lt;/a&gt;原理&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;G：协程，P：processor，M：系统线程，M绑定一个P才能执行G；&lt;/li&gt;
&lt;li&gt;P的数据是控制着Go的并发能力，由环境变量&lt;code&gt;GOMAXPROCESS&lt;/code&gt;控制数量，线程数一般不配置，但也有一个底层的函数可设置（&lt;code&gt;SetMaxThreads&lt;/code&gt;），M默认上限是10000；&lt;/li&gt;
&lt;li&gt;P的存在是为了实现G的调度和M&amp;#x2F;G之间M:N的关系；&lt;/li&gt;
&lt;li&gt;P有一个local queue，无锁，当P的local queue无G时，会从Global queue取，此时需要用锁，而如果Global queue也无G时，会走work stealing，尝试从其它P的local queue上找G运行；&lt;/li&gt;
&lt;li&gt;G的抢占调度是基于时间片，即如果G长时间连续运行超过10ms，则会通过系统信号SIGURG强制调度该协程；&lt;/li&gt;
&lt;li&gt;当G发生阻塞，如系统调用IO操作时，会创建一个新的线程或利用一个空闲的线程来绑定当前P，以继续执行P上的G，而旧线程进入阻塞睡眠状态；&lt;/li&gt;
&lt;li&gt;P的local queue放的G的数量上限为256；&lt;/li&gt;
&lt;li&gt;当因IO阻塞进入idle状态的线程，在G系统调用结束时，因此时M上没有P了，而G的执行一定需要P，所以会尝试先去获取一个idle的P来执行，如找到了，则会将该G放到该P的local queue中，如果没有，则M会进入idle线程队列，G会放到Global queue中；&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="Go" scheme="https://so2bin.github.io/tags/Go/"/>
    
    <category term="GMP" scheme="https://so2bin.github.io/tags/GMP/"/>
    
    <category term="Scheduler" scheme="https://so2bin.github.io/tags/Scheduler/"/>
    
  </entry>
  
  <entry>
    <title>Python GC</title>
    <link href="https://so2bin.github.io/2024/01/28/Python/Python%20GC/"/>
    <id>https://so2bin.github.io/2024/01/28/Python/Python%20GC/</id>
    <published>2024-01-28T09:01:31.000Z</published>
    <updated>2024-01-28T09:01:44.378Z</updated>
    
    
    
    
    
    <category term="Python" scheme="https://so2bin.github.io/tags/Python/"/>
    
    <category term="GC" scheme="https://so2bin.github.io/tags/GC/"/>
    
  </entry>
  
  <entry>
    <title>Go GC</title>
    <link href="https://so2bin.github.io/2024/01/28/Go/Go%20GC/"/>
    <id>https://so2bin.github.io/2024/01/28/Go/Go%20GC/</id>
    <published>2024-01-28T09:00:47.000Z</published>
    <updated>2024-01-28T10:58:19.124Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Go1-5-标记删除-Mark-and-Sweep"><a href="#Go1-5-标记删除-Mark-and-Sweep" class="headerlink" title="Go1.5 标记删除(Mark and Sweep)"></a>Go1.5 标记删除(Mark and Sweep)</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><ul><li>进入STW(Stop The World)阶段</li><li>遍历整个heap，排查未被引用的对象，并标记；</li><li>暂停STW</li><li>执行Sweep清除</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>STW会让Go程序暂停，程序出现卡顿</li><li>标记需要扫描整个heap</li><li>清除数据会产生heap碎片</li></ul><h2 id="Go1-8-三色标记法-写入屏障"><a href="#Go1-8-三色标记法-写入屏障" class="headerlink" title="Go1.8 三色标记法 + 写入屏障"></a>Go1.8 三色标记法 + 写入屏障</h2><h3 id="三色标记法"><a href="#三色标记法" class="headerlink" title="三色标记法"></a>三色标记法</h3><h4 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h4><ul><li>维护三个标记表：<code>白色</code>，<code>灰色</code>，<code>黑色</code></li><li>程序初始时，所有对象标记为白色</li><li>从Root Set开发，每次只遍历一层，标记该层的对象为<code>灰色</code>（对象从<code>白色</code>表中移到<code>灰色</code>表）</li><li>然后，以<code>灰色</code>标记表开始，遍历该表中的对象，将从这些对象开始的可达到对象（走一步），标记为<code>灰色</code>，同时<code>灰色</code>表中已经遍历过的对象，标记为<code>黑色</code>（对象从<code>灰色</code>表中移到<code>黑色</code>表）</li><li>循环上一步，继续遍历<code>灰色</code>表，直到<code>灰色</code>表中无任何对象</li><li>剩下的白色即为需要删除的对象</li></ul><blockquote><p>上述过程，仍然需要STW，否则会出现并发导致的引用关系错乱，导致资源被错误的GC删除</p></blockquote><span id="more"></span><h3 id="写入屏障"><a href="#写入屏障" class="headerlink" title="写入屏障"></a>写入屏障</h3><h4 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h4><ul><li><p>三色标记被破坏的情况有两种：</p><ol><li>一个白色对象被黑色对象引用</li><li>灰色对象与它的可达白色对象关系遭到破坏（灰色同时丢失了该白色）</li></ol></li><li><p>谷歌提出了两个三色不变色：</p><ol><li>强三色不变性：强制性不允许黑色引用白色</li><li>弱三色不变性：黑色可以引用白色，但白色对像存在其它灰色（可非直接上级节点）引用该白色对象</li></ol></li><li><p>只要三色不变色满足上述条件之一，即可解决三色标记必须使用STW问题</p></li></ul><h3 id="插入写屏障"><a href="#插入写屏障" class="headerlink" title="插入写屏障"></a>插入写屏障</h3><h4 id="插入屏障原理"><a href="#插入屏障原理" class="headerlink" title="插入屏障原理"></a>插入屏障原理</h4><ul><li>对堆对象，在对象被引用时触发，A引用B时，B对象标记为<code>灰色</code></li><li>对栈对象，对象被引用时，不触发该过程以避免该标记过程对程序性能的影响；</li><li>此时在GC过程，遍历<code>灰色</code>表的过程，不需要STW，直到<code>灰色</code>表为空结束</li><li>而此时栈上<code>黑色</code>对象可能引用<code>白色</code>对象，为解决这个问题，需要在上一步<code>灰色</code>表为空时，执行一些STW，并将栈上所有对象都标记为白色，并对这些对象重新执行一遍扫描；</li></ul><blockquote><p>缺点：在结束时，还需要额外扫描一次栈上的对象引用链，这会引入10-100ms；</p></blockquote><h4 id="删除屏障原理"><a href="#删除屏障原理" class="headerlink" title="删除屏障原理"></a>删除屏障原理</h4><ul><li>被删除对象，如果自身为<code>灰色</code>或<code>白色</code>，则会被标记为<code>灰色</code>；</li><li>该过程是满足弱三色不变式，保证<code>灰色</code>对象到<code>白色</code>对象的路径不会断；</li></ul><blockquote><p>缺点：被删除对象，当前这一轮GC不会被删掉（因为会被转换成黑色），但会在下轮GC中删掉；</p></blockquote><h3 id="混合写屏障"><a href="#混合写屏障" class="headerlink" title="混合写屏障"></a>混合写屏障</h3><h4 id="原理-3"><a href="#原理-3" class="headerlink" title="原理"></a>原理</h4><ul><li>GC开始时，扫描栈上的可达对象全部标记为<code>黑色</code>（期间不需要STW）</li><li>GC期间，任何在栈上创建的新对象，均为<code>黑色</code></li><li>被删除对象，被标记为<code>灰色</code></li><li>被添加的对象标记为<code>灰色</code></li></ul><blockquote><p>满足变形的弱三色不变色，混合了插入，删除写屏障的优点</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Go1-5-标记删除-Mark-and-Sweep&quot;&gt;&lt;a href=&quot;#Go1-5-标记删除-Mark-and-Sweep&quot; class=&quot;headerlink&quot; title=&quot;Go1.5 标记删除(Mark and Sweep)&quot;&gt;&lt;/a&gt;Go1.5 标记删除(Mark and Sweep)&lt;/h2&gt;&lt;h3 id=&quot;原理&quot;&gt;&lt;a href=&quot;#原理&quot; class=&quot;headerlink&quot; title=&quot;原理&quot;&gt;&lt;/a&gt;原理&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;进入STW(Stop The World)阶段&lt;/li&gt;
&lt;li&gt;遍历整个heap，排查未被引用的对象，并标记；&lt;/li&gt;
&lt;li&gt;暂停STW&lt;/li&gt;
&lt;li&gt;执行Sweep清除&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;缺点&quot;&gt;&lt;a href=&quot;#缺点&quot; class=&quot;headerlink&quot; title=&quot;缺点&quot;&gt;&lt;/a&gt;缺点&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;STW会让Go程序暂停，程序出现卡顿&lt;/li&gt;
&lt;li&gt;标记需要扫描整个heap&lt;/li&gt;
&lt;li&gt;清除数据会产生heap碎片&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Go1-8-三色标记法-写入屏障&quot;&gt;&lt;a href=&quot;#Go1-8-三色标记法-写入屏障&quot; class=&quot;headerlink&quot; title=&quot;Go1.8 三色标记法 + 写入屏障&quot;&gt;&lt;/a&gt;Go1.8 三色标记法 + 写入屏障&lt;/h2&gt;&lt;h3 id=&quot;三色标记法&quot;&gt;&lt;a href=&quot;#三色标记法&quot; class=&quot;headerlink&quot; title=&quot;三色标记法&quot;&gt;&lt;/a&gt;三色标记法&lt;/h3&gt;&lt;h4 id=&quot;原理-1&quot;&gt;&lt;a href=&quot;#原理-1&quot; class=&quot;headerlink&quot; title=&quot;原理&quot;&gt;&lt;/a&gt;原理&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;维护三个标记表：&lt;code&gt;白色&lt;/code&gt;，&lt;code&gt;灰色&lt;/code&gt;，&lt;code&gt;黑色&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;程序初始时，所有对象标记为白色&lt;/li&gt;
&lt;li&gt;从Root Set开发，每次只遍历一层，标记该层的对象为&lt;code&gt;灰色&lt;/code&gt;（对象从&lt;code&gt;白色&lt;/code&gt;表中移到&lt;code&gt;灰色&lt;/code&gt;表）&lt;/li&gt;
&lt;li&gt;然后，以&lt;code&gt;灰色&lt;/code&gt;标记表开始，遍历该表中的对象，将从这些对象开始的可达到对象（走一步），标记为&lt;code&gt;灰色&lt;/code&gt;，同时&lt;code&gt;灰色&lt;/code&gt;表中已经遍历过的对象，标记为&lt;code&gt;黑色&lt;/code&gt;（对象从&lt;code&gt;灰色&lt;/code&gt;表中移到&lt;code&gt;黑色&lt;/code&gt;表）&lt;/li&gt;
&lt;li&gt;循环上一步，继续遍历&lt;code&gt;灰色&lt;/code&gt;表，直到&lt;code&gt;灰色&lt;/code&gt;表中无任何对象&lt;/li&gt;
&lt;li&gt;剩下的白色即为需要删除的对象&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;上述过程，仍然需要STW，否则会出现并发导致的引用关系错乱，导致资源被错误的GC删除&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    
    <category term="GC" scheme="https://so2bin.github.io/tags/GC/"/>
    
    <category term="Go" scheme="https://so2bin.github.io/tags/Go/"/>
    
  </entry>
  
  <entry>
    <title>Go 内存模型</title>
    <link href="https://so2bin.github.io/2024/01/28/Go/mem/"/>
    <id>https://so2bin.github.io/2024/01/28/Go/mem/</id>
    <published>2024-01-28T09:00:47.000Z</published>
    <updated>2024-02-21T11:02:07.691Z</updated>
    
    <content type="html"><![CDATA[<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><h3 id="分层架构"><a href="#分层架构" class="headerlink" title="分层架构"></a>分层架构</h3><ul><li>Go的页单元大小 ：8KB，linux操作系统：4KB；</li><li>如下图所示，分了三层来管理<ul><li>mcache: 每个P一个，为每个mspan额外缓存了一个，无锁；</li><li>mcentral: 分了67个span，从8B-32B，细粒度锁；</li><li>mheap: 全局唯一；<img src="/2024/01/28/Go/mem/go-mem-arch.png" width="60%" height="60%" style="margin: 0 auto;" alt="go-mem-arch"><span id="more"></span><img src="/2024/01/28/Go/mem/go-mem-arch2.png" width="60%" height="60%" style="margin: 0 auto;" alt="go-mem-arch2"></li></ul></li></ul><h3 id="三类对象"><a href="#三类对象" class="headerlink" title="三类对象"></a>三类对象</h3><ul><li>tiny 微对象，&lt;&#x3D;8B，通过P的专属tinyAllocator分配，依次从：mcache tiny -&gt; mcache span -&gt; mcentral -&gt; mheap -&gt; VM分配；</li><li>small 小对象，&lt;&#x3D;32KB， 依次从：mcache span -&gt; mcentral -&gt; mheap -&gt; VM分配；</li><li>large 大对象，&gt;32KB，依次从：mcentral -&gt; mheap -&gt; VM分配；</li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;架构&quot;&gt;&lt;a href=&quot;#架构&quot; class=&quot;headerlink&quot; title=&quot;架构&quot;&gt;&lt;/a&gt;架构&lt;/h2&gt;&lt;h3 id=&quot;分层架构&quot;&gt;&lt;a href=&quot;#分层架构&quot; class=&quot;headerlink&quot; title=&quot;分层架构&quot;&gt;&lt;/a&gt;分层架构&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Go的页单元大小 ：8KB，linux操作系统：4KB；&lt;/li&gt;
&lt;li&gt;如下图所示，分了三层来管理&lt;ul&gt;
&lt;li&gt;mcache: 每个P一个，为每个mspan额外缓存了一个，无锁；&lt;/li&gt;
&lt;li&gt;mcentral: 分了67个span，从8B-32B，细粒度锁；&lt;/li&gt;
&lt;li&gt;mheap: 全局唯一；&lt;img src=&quot;/2024/01/28/Go/mem/go-mem-arch.png&quot; width=&quot;60%&quot; height=&quot;60%&quot; style=&quot;margin: 0 auto;&quot; alt=&quot;go-mem-arch&quot;&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</summary>
    
    
    
    
    <category term="Go" scheme="https://so2bin.github.io/tags/Go/"/>
    
    <category term="内存模型" scheme="https://so2bin.github.io/tags/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>CNCF视频清单</title>
    <link href="https://so2bin.github.io/2024/01/26/cloud-native/CNCF/"/>
    <id>https://so2bin.github.io/2024/01/26/cloud-native/CNCF/</id>
    <published>2024-01-26T03:00:48.000Z</published>
    <updated>2024-01-26T03:01:40.574Z</updated>
    
    <content type="html"><![CDATA[<h2 id="CNCF视频清单"><a href="#CNCF视频清单" class="headerlink" title="CNCF视频清单"></a>CNCF视频清单</h2><ul><li><a href="https://youtu.be/22W1yrEJjtQ?si=6B8RvYzrPlZZPlEg">FederaHPA on Karmada</a></li><li><a href="https://youtu.be/IHnRH_hWAxM?si=KH7AP5myug0K7fFw">Serverless AIGC平台</a></li><li><a href="https://youtu.be/OO7zpyf7fgs?si=9RKYqqCg9fnviDRp">Vocalno with AI workload</a></li><li><a href="https://youtu.be/vJTSJYc0vSc?si=hrTcd9DIQPi6mX6a">Dragonfly for AI model Distribution</a></li><li><a href="https://youtu.be/mLsIcZyop5o?si=Gom34Aq8GFTeJwF6">SIG kube-scheduler</a></li><li><a href="https://youtu.be/4Bs9Pgn4z2w?si=EYNBj2PIcAsnFFuf">controller-runtime新特性</a></li><li><a href="https://youtu.be/t3YFv20Ulnc?si=vzsWl1H8neD6natV">Prometheus Deep Dive</a></li><li><a href="https://youtu.be/gwV7bFL91fQ?si=xD_Fptga9fE1hWd9">containerd</a></li><li><a href="https://youtu.be/7pQZyYEz1l4?si=k7Dc9XZ1-HP2oXwp">Prometheus高性能远程存储</a></li><li><a href="https://youtu.be/e4GA5e-C7n0?si=8dAbgg5hzbU4jwjM">多集群流量调度based on eBPF</a></li><li><a href="https://youtu.be/oMz1sbCd6-Q?si=p_brZ49YRmZ1Olyb">服务网格负载均衡</a></li><li><a href="https://youtu.be/hHZxjH8u84s?si=0aFnDTnGXVCTY50l">k8s + RoCEv2构建AI训练集群</a></li><li><a href="https://youtu.be/U6Jg06JeCKM?si=02r86xmRZisADhJ6">基于WASM部署轻量级AI服务</a></li><li><a href="https://youtu.be/JOio7Nps_II?si=4HLeqKJGOloJfZtt">CubeFS with AI</a></li><li><a href="https://youtu.be/rL199Ee55cU?si=BD61mCApi_NyBiAX">new with GRPC</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;CNCF视频清单&quot;&gt;&lt;a href=&quot;#CNCF视频清单&quot; class=&quot;headerlink&quot; title=&quot;CNCF视频清单&quot;&gt;&lt;/a&gt;CNCF视频清单&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/22W1yrEJjtQ?si</summary>
      
    
    
    
    
    <category term="CNCF" scheme="https://so2bin.github.io/tags/CNCF/"/>
    
  </entry>
  
  <entry>
    <title>kubeadm初始化k8s</title>
    <link href="https://so2bin.github.io/2024/01/24/tutorials/kubeadm/"/>
    <id>https://so2bin.github.io/2024/01/24/tutorials/kubeadm/</id>
    <published>2024-01-24T03:26:41.000Z</published>
    <updated>2024-01-26T02:33:19.684Z</updated>
    
    <content type="html"><![CDATA[<h2 id="init"><a href="#init" class="headerlink" title="init"></a>init</h2><h3 id="节点环境准备"><a href="#节点环境准备" class="headerlink" title="节点环境准备"></a>节点环境准备</h3><ul><li>关闭swap<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看</span></span><br><span class="line">sudo swapon --show</span><br><span class="line"><span class="comment"># 关闭</span></span><br><span class="line">sudo swapoff -a</span><br></pre></td></tr></table></figure></li><li>配置其它：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sudo vim /etc/sysctl.d/k8s.conf</span></span><br><span class="line"><span class="comment"># 添加： </span></span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导出配置</span></span><br><span class="line">containerd config default &gt; /etc/containerd/config.toml</span><br><span class="line"><span class="comment"># 修改配置</span></span><br><span class="line">[plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.containerd.runtimes.runc]</span><br><span class="line">  ...</span><br><span class="line">  [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.containerd.runtimes.runc.options]</span><br><span class="line">    SystemdCgroup = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行命令</span></span><br><span class="line">sudo modprobe br_netfilter</span><br><span class="line">sudo systemctl restart containerd</span><br></pre></td></tr></table></figure></li><li>按正常流程安装完docker, containerd组件</li></ul><span id="more"></span><h3 id="默认安装"><a href="#默认安装" class="headerlink" title="默认安装"></a>默认安装</h3><ul><li>kubeadm初始化<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 以containerd作为运行时</span></span><br><span class="line">NODENAME=$(hostname -s) \</span><br><span class="line">&amp;&amp; KUBERNETES_VERSION=<span class="string">&quot;v1.25.6&quot;</span> \</span><br><span class="line">&amp;&amp; MASTER_IP=$(ip addr show enp3s0f0 | grep <span class="string">&quot;inet\b&quot;</span> | awk <span class="string">&#x27;&#123;print $2&#125;&#x27;</span> | <span class="built_in">cut</span> -d/ -f1)</span><br><span class="line">sudo kubeadm init --image-repository=registry.aliyuncs.com/google_containers  \</span><br><span class="line"> --pod-network-cidr=<span class="string">&quot;192.168.0.0/16&quot;</span> \</span><br><span class="line"> --service-cidr=<span class="string">&quot;10.96.0.0/12&quot;</span>  \</span><br><span class="line"> --kubernetes-version <span class="variable">$KUBERNETES_VERSION</span>  \</span><br><span class="line"> --apiserver-advertise-address <span class="variable">$MASTER_IP</span>  \</span><br><span class="line"> --node-name <span class="variable">$NODENAME</span> \</span><br><span class="line"> --cri-socket unix:///run/containerd/containerd.sock \</span><br><span class="line"> --control-plane-endpoint=atms-01.vm</span><br><span class="line"></span><br><span class="line">kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/tigera-operator.yaml \</span><br><span class="line">    -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/custom-resources.yaml</span><br><span class="line"></span><br><span class="line">kubectl  taint node atms-02 node-role.kubernetes.io/control-plane:NoSchedule-</span><br></pre></td></tr></table></figure></li></ul><h3 id="修改CIDR"><a href="#修改CIDR" class="headerlink" title="修改CIDR"></a>修改CIDR</h3><ul><li><p>在一些k8s安装要求下，可能需要修改pod CIDR或service CIDR，可通过如下方式安装：</p><blockquote><p>当前修改service CIDR没生效，所以只演示修改pod CIDR</p></blockquote></li><li><p>kubeadm初始化 </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 以containerd作为运行时</span></span><br><span class="line">NODENAME=$(hostname -s) \</span><br><span class="line">&amp;&amp; KUBERNETES_VERSION=<span class="string">&quot;v1.25.6&quot;</span> \</span><br><span class="line">&amp;&amp; MASTER_IP=$(ip addr show enp0s31f6 | grep <span class="string">&quot;inet\b&quot;</span> | awk <span class="string">&#x27;&#123;print $2&#125;&#x27;</span> | <span class="built_in">cut</span> -d/ -f1)</span><br><span class="line">sudo kubeadm init --image-repository=registry.aliyuncs.com/google_containers  \</span><br><span class="line"> --pod-network-cidr=<span class="string">&quot;192.169.0.0/16&quot;</span> \</span><br><span class="line"> --service-cidr=<span class="string">&quot;10.96.0.0/12&quot;</span>  \</span><br><span class="line"> --kubernetes-version <span class="variable">$KUBERNETES_VERSION</span>  \</span><br><span class="line"> --apiserver-advertise-address <span class="variable">$MASTER_IP</span>  \</span><br><span class="line"> --node-name <span class="variable">$NODENAME</span> \</span><br><span class="line"> --cri-socket unix:///run/containerd/containerd.sock \</span><br><span class="line"> --control-plane-endpoint=atms-02.vm</span><br><span class="line"></span><br><span class="line">kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/tigera-operator.yaml </span><br><span class="line"></span><br><span class="line">kubectl  taint node atms-02 node-role.kubernetes.io/control-plane:NoSchedule-</span><br></pre></td></tr></table></figure></li><li><p>修改calico：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># wget https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/custom-resources.yaml</span></span><br><span class="line"><span class="comment"># 修改内容如下：</span></span><br><span class="line"><span class="comment"># 自定义calico custom-resource.yaml配置自定义CIDR</span></span><br><span class="line"><span class="comment"># This section includes base Calico installation configuration.</span></span><br><span class="line"><span class="comment"># For more information, see: https://projectcalico.docs.tigera.io/master/reference/installation/api#operator.tigera.io/v1.Installation</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">operator.tigera.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Installation</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># Configures Calico networking.</span></span><br><span class="line">  <span class="attr">calicoNetwork:</span></span><br><span class="line">    <span class="comment"># <span class="doctag">Note:</span> The ipPools section cannot be modified post-install.</span></span><br><span class="line">    <span class="attr">ipPools:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">blockSize:</span> <span class="number">26</span></span><br><span class="line">      <span class="attr">cidr:</span> <span class="number">192.169</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line">      <span class="attr">encapsulation:</span> <span class="string">VXLANCrossSubnet</span></span><br><span class="line">      <span class="attr">natOutgoing:</span> <span class="string">Enabled</span></span><br><span class="line">      <span class="attr">nodeSelector:</span> <span class="string">all()</span></span><br><span class="line">  <span class="comment">#serviceCIDRs:</span></span><br><span class="line">  <span class="comment">#- 10.97.0.0/12</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="comment"># This section configures the Calico API server.</span></span><br><span class="line"><span class="comment"># For more information, see: https://projectcalico.docs.tigera.io/master/reference/installation/api#operator.tigera.io/v1.APIServer</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">operator.tigera.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">APIServer</span> </span><br><span class="line"><span class="attr">metadata:</span> </span><br><span class="line">  <span class="attr">name:</span> <span class="string">default</span> </span><br><span class="line"><span class="attr">spec:</span> &#123;&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="reset"><a href="#reset" class="headerlink" title="reset"></a>reset</h2><ul><li>通过如下命令重置k8s节点，如在执行<code>kubeadm init</code>或<code>kubeadm join</code>后可通过下面命令重置节点：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo kubeadm reset</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;init&quot;&gt;&lt;a href=&quot;#init&quot; class=&quot;headerlink&quot; title=&quot;init&quot;&gt;&lt;/a&gt;init&lt;/h2&gt;&lt;h3 id=&quot;节点环境准备&quot;&gt;&lt;a href=&quot;#节点环境准备&quot; class=&quot;headerlink&quot; title=&quot;节点环境准备&quot;&gt;&lt;/a&gt;节点环境准备&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;关闭swap&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 查看&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo swapon --show&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 关闭&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo swapoff -a&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;li&gt;配置其它：&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# sudo vim /etc/sysctl.d/k8s.conf&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 添加： &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;net.ipv4.ip_forward = 1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 导出配置&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;containerd config default &amp;gt; /etc/containerd/config.toml&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 修改配置&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[plugins.&lt;span class=&quot;string&quot;&gt;&amp;quot;io.containerd.grpc.v1.cri&amp;quot;&lt;/span&gt;.containerd.runtimes.runc]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  ...&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  [plugins.&lt;span class=&quot;string&quot;&gt;&amp;quot;io.containerd.grpc.v1.cri&amp;quot;&lt;/span&gt;.containerd.runtimes.runc.options]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    SystemdCgroup = &lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 执行命令&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo modprobe br_netfilter&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo systemctl restart containerd&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;li&gt;按正常流程安装完docker, containerd组件&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="k8s" scheme="https://so2bin.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Karmada</title>
    <link href="https://so2bin.github.io/2024/01/18/cloud-native/karmada/"/>
    <id>https://so2bin.github.io/2024/01/18/cloud-native/karmada/</id>
    <published>2024-01-18T12:07:37.000Z</published>
    <updated>2024-03-04T12:46:27.258Z</updated>
    
    <content type="html"><![CDATA[<h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><ul><li><a href="https://cloud.tencent.com/developer/article/1804669">kubefed v2</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzIzNTU1MjIxMQ==&amp;mid=2247483886&amp;idx=1&amp;sn=d397c17088a6a5c2516d7a77acb961e6&amp;chksm=e8e42d52df93a44416c4f250c581158e15d44ba17bc11f5abcd310d59bf9c9fe5fef5aa0e4b8&amp;scene=21#wechat_redirect">Kubernetes 多集群管理：Kubefed</a></li><li><a href="https://www.cnblogs.com/vivotech/p/17684105.html">vivo Karmada实践</a></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><ul><li>karmada是由华为开源的云原生多集群容器编排平台，在kubernetes Federation v1, v2（<code>kubefed</code>）的基础上发展而来，吸取了其经验和教训，<code>kubefed</code>项目目前已经被放弃；</li><li>其特点是在保存原有k8s资源定义API不变的情况下，通过添加与多云应用资源编排相关的一套新的API和控制面板组件，为用户提供多云&#x2F;多集群容器部署，实现扩展、高可用等目标；</li><li>如下图所示为<code>kubefed</code> v2接入vs时需要定义的CRD，因此接入<code>kubefed</code>是需要对原始k8s资源进行改造，对用户不友好：<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">types.kubefed.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">FederatedVirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">service-route</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">placement:</span></span><br><span class="line">    <span class="attr">clusters:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cluster1</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cluster2</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cluster3</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">service-route</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">gateways:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">service-gateway</span></span><br><span class="line">      <span class="attr">hosts:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;*&#x27;</span></span><br><span class="line">      <span class="attr">http:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">match:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">uri:</span></span><br><span class="line">            <span class="attr">prefix:</span> <span class="string">/</span></span><br><span class="line">        <span class="attr">route:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">            <span class="attr">host:</span> <span class="string">service-a-1</span></span><br><span class="line">            <span class="attr">port:</span></span><br><span class="line">              <span class="attr">number:</span> <span class="number">3000</span></span><br></pre></td></tr></table></figure></li></ul><span id="more"></span><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><ul><li>跨集群的pod网络通信可以接入如<code>Submariner</code>这种开源方案</li><li>In order to prevent routing conflicts, Pod and Service CIDRs of clusters need non-overlapping</li><li>要开启multi-cluster service，需要安装<code>ServiceExport</code>和<code>ServiceImport</code></li></ul><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><h3 id="架构-1"><a href="#架构-1" class="headerlink" title="架构"></a>架构</h3><ul><li>核心组件有：<code>karmada apiserver</code>, <code>karmada controller manager</code>, <code>karmada scheduler</code>；</li><li><code>karmada apiserver</code>是在k8s apiserver的基础上开发的，所以功能与k8s apiserver类似；</li><li><code>karmada scheduler</code>是集群调度器，即完成在资源下发阶段，为资源选择合适集群的目标；</li><li><code>karmada controller manager</code>如下图所示，集成了4个controller：<ul><li><code>Cluster Controller</code> 将k8s集群连接到karmada，为每个集群都创建了一个<a href="https://github.com/karmada-io/api/blob/81a2cd59ba32a54fd45b02253e9d2d82e1be12cf/cluster/v1alpha1/types.go#L43">Cluster资源对象</a>，来管理集群的生命周期；</li><li><code>Policy Controller</code> 监视<code>PropagationPolicy</code>对象，当该CRD创建时，controller会选择与<code>resourceSelector</code>匹配的一组资源，为每个单独的联邦资源对象创建<code>ResourceBinding</code>对象；</li><li><code>Binding Controller</code> 监视<code>ResourceBinding</code>对象，为每个带有单个资源manifest的集群创建一个<a href="https://github.com/karmada-io/api/blob/81a2cd59ba32a54fd45b02253e9d2d82e1be12cf/work/v1alpha1/work_types.go#L43"><code>Work</code>对象</a>；</li><li><code>Executioin Controller</code> 监视<code>Work</code>对象，当资源创建时，controller会把资源下发到成员集群中；</li></ul></li></ul><p><img src="/2024/01/18/cloud-native/karmada/arch.png" alt="Arch"></p><h3 id="资源下发四个阶段"><a href="#资源下发四个阶段" class="headerlink" title="资源下发四个阶段"></a>资源下发四个阶段</h3><ul><li>karmada下的资源配置下发会经历4个阶段：<code>Resource Template</code> -&gt; <code>Propagation Policy</code> -&gt; <code>Resource Binding</code> -&gt; <code>Override Policy</code>；</li><li><code>Resource Template</code>阶段：是定义资源模板，其可直接套用原生k8s的配置，无需进行改造；</li><li><code>Propagation Policy</code>阶段：为通过PropagationPolicy API来定义多集群的调度要求，<a href="https://github.com/karmada-io/api/blob/81a2cd59ba32a54fd45b02253e9d2d82e1be12cf/policy/v1alpha1/propagation_types.go#L49">CRD定义源码在这</a>；<ul><li>支持1:N的策略映射机制，无需为每个联邦应用都标明调度约束；</li><li>使用默认策略的情况下，用户可以直接与k8s API交互；</li></ul></li><li><code>Resource Binding</code>阶段：此时应用会为每个联邦应用创建一个ResourceBinding资源，用于关联其PropagationPolicy资源，该<a href="https://github.com/karmada-io/api/blob/81a2cd59ba32a54fd45b02253e9d2d82e1be12cf/work/v1alpha2/binding_types.go#L58">CRD定义源码在这</a></li><li><code>Override Policy</code>阶段：为每个ResourceBinding资源执行集群级别的差异化配置改写，其<a href="https://github.com/karmada-io/api/blob/81a2cd59ba32a54fd45b02253e9d2d82e1be12cf/policy/v1alpha1/override_types.go#L49">CRD定义源码在这</a></li></ul><p><img src="/2024/01/18/cloud-native/karmada/resource-apply.png" alt="Karmada资源下发"></p><h2 id="组件特性"><a href="#组件特性" class="headerlink" title="组件特性"></a>组件特性</h2><h3 id="PropagationPolicy"><a href="#PropagationPolicy" class="headerlink" title="PropagationPolicy"></a>PropagationPolicy</h3><h3 id="重调度：karmada-descheduler-and-karmada-scheduler-estimator"><a href="#重调度：karmada-descheduler-and-karmada-scheduler-estimator" class="headerlink" title="重调度：karmada-descheduler and karmada-scheduler-estimator"></a>重调度：<code>karmada-descheduler</code> and <code>karmada-scheduler-estimator</code></h3><h4 id="karmada-descheduler"><a href="#karmada-descheduler" class="headerlink" title="karmada-descheduler"></a><code>karmada-descheduler</code></h4><ul><li>可根据成员集群内实例状态变化，主动触发重调度；</li></ul><h4 id="karmada-scheduler-estimator"><a href="#karmada-scheduler-estimator" class="headerlink" title="karmada-scheduler-estimator"></a><code>karmada-scheduler-estimator</code></h4><ul><li>为调度器提供更精确的成员集群运行实例的期望状态；</li></ul><h2 id="Karmada-Install"><a href="#Karmada-Install" class="headerlink" title="Karmada Install"></a>Karmada Install</h2><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><ol><li>安装k8s；</li><li>如k8s主机是配置的域名，因karmada-controller需要通过该域名如<code>atms-00.vm</code>访问目标集群，所以需要在karmada机器的coredns上配置如下IP映射：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#  kubectl edit cm -n kube-system coredns  -o yaml</span><br><span class="line">        hosts &#123;</span><br><span class="line">          10.13.149.88 atms-01.vm</span><br><span class="line">          10.13.148.34 atms-02.vm</span><br><span class="line"></span><br><span class="line">          fallthrough</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="install"><a href="#install" class="headerlink" title="install"></a>install</h3><ol><li><p>安装cli：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -s https://raw.githubusercontent.com/karmada-io/karmada/master/hack/install-cli.sh | sudo INSTALL_CLI_VERSION=1.8.1 bash</span><br><span class="line">curl -s https://raw.githubusercontent.com/karmada-io/karmada/master/hack/install-cli.sh | sudo INSTALL_CLI_VERSION=1.8.1 bash -s kubectl-karmada</span><br></pre></td></tr></table></figure></li><li><p>初始化karmada</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo kubectl karmada init --kubeconfig ~/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment">#mkdir -p $HOME/.kube</span></span><br><span class="line">sudo <span class="built_in">cp</span> -i /etc/karmada/karmada-apiserver.config <span class="variable">$HOME</span>/.kube/karmada-apiserver.config</span><br><span class="line"><span class="comment"># sudo cp -i /etc/karmada/karmada-apiserver.config $HOME/.kube/karmada-apiserver.config</span></span><br><span class="line">sudo <span class="built_in">chown</span> $(<span class="built_in">id</span> -u):$(<span class="built_in">id</span> -g) <span class="variable">$HOME</span>/.kube/karmada-apiserver.config</span><br></pre></td></tr></table></figure></li><li><p>Karmada有两个context: <code>karmada-apiserver</code>和<code>karmada-host</code>，可通过<code>kubectl config view</code>查看所有集群：</p></li></ol><p><strong><code>karmada-apiserver</code></strong></p><ul><li>切换Context：<code>kubectl config use-context karmada-apiserver --kubeconfig ~/.kube/karmada-apiserver.config</code></li><li>该Context是与Karmada控制面板交互时使用的主要<code>kubeconfig</code>；</li></ul><p><strong><code>karmada-host</code></strong></p><ul><li>切换Context：<code>kubectl config use-context karmada-host --kubeconfig ~/.kube/karmada-apiserver.config</code></li><li>该Context仅用于调试Karmada对<code>hostcluster</code>的安装；</li></ul><ol start="4"><li>join集群<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先更改当前member集群的名</span></span><br><span class="line">kubectl config rename-context kubernetes-admin@kubernetes atms-01</span><br><span class="line"></span><br><span class="line"><span class="comment"># register</span></span><br><span class="line">kubectl karmada --kubeconfig ~/.kube/karmada-apiserver.config  <span class="built_in">join</span> atms-01 --cluster-kubeconfig=<span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># check</span></span><br><span class="line">kubectl get clusters --kubeconfig ~/.kube/karmada-apiserver.config</span><br></pre></td></tr></table></figure></li><li>OK</li></ol><h3 id="uninstall"><a href="#uninstall" class="headerlink" title="uninstall"></a>uninstall</h3><ul><li>命令如下：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl --kubeconfig /etc/karmada/karmada-apiserver.config get clusters</span><br><span class="line">sudo <span class="built_in">rm</span> -rf /var/lib/karmada-etcd</span><br></pre></td></tr></table></figure></li></ul><h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><ul><li>Karmada集群中，<code>Namespace</code>资源是会被自动分发到集群成员中，这个功能是由<code>Karmada-controller-manager</code>组件中的<code>namespace</code> controller负责，可以通过配置<code>Karmada</code>控制器来进行配置，配置后，用户可以通过<code>ClusterPropagationPolicy</code>资源指定<code>Namespace</code>资源的分发策略；(<a href="https://karmada.io/zh/docs/faq/">参考</a>)</li></ul><h2 id="Resource-Propagating"><a href="#Resource-Propagating" class="headerlink" title="Resource Propagating"></a>Resource Propagating</h2><blockquote><p><a href="https://karmada.io/zh/docs/userguide/scheduling/resource-propagating">https://karmada.io/zh/docs/userguide/scheduling/resource-propagating</a></p></blockquote><h3 id="Propagation-API"><a href="#Propagation-API" class="headerlink" title="Propagation API"></a>Propagation API</h3><ul><li>Karmada提供两种资源分发API：<code>PropagationPolicy</code>和<code>ClusterPropagationPolicy</code></li><li><code>PropagationPolicy</code>：只能作用于同一命名空间下资源的分发策略；</li><li><code>ClusterPropagationPolicy</code>：可以作用于所有命名空间下资源的分发策略；</li><li>更新<code>PropagationPolicy</code>的目标集群，会立刻对资源产生作用，如<code>PropagationPolicy</code>先将deployment分发到集群A，更新<code>PropagationPolicy</code>将deployment分发到集群B后，集群A的pod会被清理掉；</li><li>删除<code>PropagationPolicy</code>不会自动删除分发出去的资源；</li></ul><h3 id="Cluster-Selector"><a href="#Cluster-Selector" class="headerlink" title="Cluster Selector"></a>Cluster Selector</h3><ul><li><p>LabelSelector</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy.karmada.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PropagationPolicy</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">placement:</span></span><br><span class="line">    <span class="attr">clusterAffinity:</span></span><br><span class="line">      <span class="attr">labelSelector:</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">location:</span> <span class="string">us</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy.karmada.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PropagationPolicy</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">placement:</span></span><br><span class="line">    <span class="attr">clusterAffinity:</span></span><br><span class="line">      <span class="attr">labelSelector:</span></span><br><span class="line">        <span class="attr">matchExpressions:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">location</span></span><br><span class="line">          <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">          <span class="attr">values:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">us</span></span><br></pre></td></tr></table></figure></li><li><p>FieldSelector</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">placement:</span></span><br><span class="line">    <span class="attr">clusterAffinity:</span></span><br><span class="line">      <span class="attr">fieldSelector:</span></span><br><span class="line">        <span class="attr">matchExpressions:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">region</span></span><br><span class="line">          <span class="attr">operator:</span> <span class="string">NotIn</span></span><br><span class="line">          <span class="attr">values:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">cn-south-1</span></span><br></pre></td></tr></table></figure></li><li><p>ClusterNames</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">placement:</span></span><br><span class="line">    <span class="attr">clusterAffinity:</span></span><br><span class="line">      <span class="attr">clusterNames:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">member1</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">member2</span></span><br></pre></td></tr></table></figure></li><li><p>ExcludeClusters</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">placement:</span></span><br><span class="line">    <span class="attr">clusterAffinity:</span></span><br><span class="line">      <span class="attr">exclude:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">member1</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">member3</span></span><br></pre></td></tr></table></figure></li><li><p>也支持基于污点的集群调度；</p></li></ul><h3 id="Replicas调度策略"><a href="#Replicas调度策略" class="headerlink" title="Replicas调度策略"></a>Replicas调度策略</h3><ul><li><code>.spec.placement.replicaScheduling</code>字段代表了处理带有<code>.replicas</code>属性的资源如<code>deployment</code>, <code>statefulsets</code>, <code>CRDs</code>的副本分发策略；（<code>CRDs</code>可以通过<a href="https://karmada.io/zh/docs/userguide/globalview/customizing-resource-interpreter/">自定义资源解释器</a>来支持）</li></ul><h4 id="两种Replicas调度类型"><a href="#两种Replicas调度类型" class="headerlink" title="两种Replicas调度类型"></a>两种Replicas调度类型</h4><blockquote><p>作用于<code>.spec.placement.replicaScheduling.replicaSchedulingType</code></p></blockquote><ol><li><code>Duplicated</code>：候选集群中副本数一样，如<code>.replicas=3</code>，则每个集群都是3个副本；</li><li><code>Divided</code>：候选集群中副本数一起划分，划分策略通过<code>.spec.placement.replicaScheduling.replicaDivisionPreference</code>字段来决定；</li></ol><h4 id="副本划分策略ReplicaDivisionPreference"><a href="#副本划分策略ReplicaDivisionPreference" class="headerlink" title="副本划分策略ReplicaDivisionPreference"></a>副本划分策略<code>ReplicaDivisionPreference</code></h4><blockquote><p>仅在<code>.replicaSchedulingType: Divided</code>时生效</p></blockquote><ol><li><code>Aggregated</code>: 根据集群资源的情况，将尽可能少的副本数划分到集群中；</li><li><code>Weighted</code>: 根据<code>WeightPreference</code>策略，按比例划分副本数；</li></ol><p><strong><code>WeightPreference</code>策略</strong></p><ol><li><code>StaticWeightList</code>: 静态权重分配；</li><li><code>DynamicWeight </code>: 根据动态权重因子来动态决定副本数，当前支持的因子有：<code>AvailableReplicas</code>，即根据集群能运行的副本数量上限，按比例在集群间划分；</li></ol><h3 id="Propagation优先级"><a href="#Propagation优先级" class="headerlink" title="Propagation优先级"></a>Propagation优先级</h3><ul><li><code>PropagationPolicy</code>的优先级 &gt; <code>ClusterPropagationPolicy</code>优先级；</li><li>显式优先级：通过字段<code>.priority: 0</code>来指定；</li><li>隐式优先级：参考文档，根据集群筛选的selector类型，分三种优先级；</li></ul><h2 id="自定义资源解释器"><a href="#自定义资源解释器" class="headerlink" title="自定义资源解释器"></a>自定义资源解释器</h2>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;资料&quot;&gt;&lt;a href=&quot;#资料&quot; class=&quot;headerlink&quot; title=&quot;资料&quot;&gt;&lt;/a&gt;资料&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://cloud.tencent.com/developer/article/1804669&quot;&gt;kubefed v2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzIzNTU1MjIxMQ==&amp;amp;mid=2247483886&amp;amp;idx=1&amp;amp;sn=d397c17088a6a5c2516d7a77acb961e6&amp;amp;chksm=e8e42d52df93a44416c4f250c581158e15d44ba17bc11f5abcd310d59bf9c9fe5fef5aa0e4b8&amp;amp;scene=21#wechat_redirect&quot;&gt;Kubernetes 多集群管理：Kubefed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.cnblogs.com/vivotech/p/17684105.html&quot;&gt;vivo Karmada实践&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;karmada是由华为开源的云原生多集群容器编排平台，在kubernetes Federation v1, v2（&lt;code&gt;kubefed&lt;/code&gt;）的基础上发展而来，吸取了其经验和教训，&lt;code&gt;kubefed&lt;/code&gt;项目目前已经被放弃；&lt;/li&gt;
&lt;li&gt;其特点是在保存原有k8s资源定义API不变的情况下，通过添加与多云应用资源编排相关的一套新的API和控制面板组件，为用户提供多云&amp;#x2F;多集群容器部署，实现扩展、高可用等目标；&lt;/li&gt;
&lt;li&gt;如下图所示为&lt;code&gt;kubefed&lt;/code&gt; v2接入vs时需要定义的CRD，因此接入&lt;code&gt;kubefed&lt;/code&gt;是需要对原始k8s资源进行改造，对用户不友好：&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;apiVersion:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;types.kubefed.io/v1beta1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;kind:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;FederatedVirtualService&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;metadata:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;service-route&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;namespace:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;default&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;spec:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;placement:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;clusters:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;cluster1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;cluster2&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;cluster3&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;template:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;metadata:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;attr&quot;&gt;name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;service-route&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;spec:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;attr&quot;&gt;gateways:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;service-gateway&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;attr&quot;&gt;hosts:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&amp;#x27;*&amp;#x27;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;attr&quot;&gt;http:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;match:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;uri:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;attr&quot;&gt;prefix:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;/&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;attr&quot;&gt;route:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;destination:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;attr&quot;&gt;host:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;service-a-1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;attr&quot;&gt;port:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;              &lt;span class=&quot;attr&quot;&gt;number:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;3000&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="k8s" scheme="https://so2bin.github.io/tags/k8s/"/>
    
    <category term="Cloud Native" scheme="https://so2bin.github.io/tags/Cloud-Native/"/>
    
    <category term="Karmada" scheme="https://so2bin.github.io/tags/Karmada/"/>
    
  </entry>
  
  <entry>
    <title>containerd</title>
    <link href="https://so2bin.github.io/2024/01/16/cloud-native/containerd/"/>
    <id>https://so2bin.github.io/2024/01/16/cloud-native/containerd/</id>
    <published>2024-01-16T14:53:39.000Z</published>
    <updated>2024-01-26T02:33:19.684Z</updated>
    
    <content type="html"><![CDATA[<h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><ul><li><p><a href="https://github.com/containerd/containerd/blob/main/docs/historical/design/architecture.md">https://github.com/containerd/containerd/blob/main/docs/historical/design/architecture.md</a></p></li><li><p><a href="https://blog.frognew.com/2021/05/relearning-container-08.html">https://blog.frognew.com/2021/05/relearning-container-08.html</a></p></li><li><p><a href="https://blog.frognew.com/2021/06/relearning-container-09.html">https://blog.frognew.com/2021/06/relearning-container-09.html</a></p></li><li><p><a href="https://blog.mobyproject.org/where-are-containerds-graph-drivers-145fc9b7255">https://blog.mobyproject.org/where-are-containerds-graph-drivers-145fc9b7255</a></p></li><li><p>高层架构<br><img src="/2024/01/16/cloud-native/containerd/top-arch.png" alt="Top Arch"></p></li></ul><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><h3 id="联合挂载"><a href="#联合挂载" class="headerlink" title="联合挂载"></a>联合挂载</h3><ul><li>由overlay filesystem提供的能力，支持将多个文件系统层叠加在一起，且只显示最顶层的文件和目录，OverlayFS是其实现，docker当前默认存储驱动为overlay2，就是基于该文件系统；</li><li>在contaierd中这个联合挂载的roofs视图是由snapshotter准备的snapshots；</li></ul><span id="more"></span><h3 id="filesystem"><a href="#filesystem" class="headerlink" title="filesystem"></a>filesystem</h3><ul><li>在容器生态中，有两种类型的文件系统：overlays filesystem, snapshoting filesystem；</li><li>AUFS, OverlayFS是overlays filesystem，通过联合挂载将多层目录合并提供给容器，提供file级差异，通常工作于常用文件系统如EXT4, XFS；</li><li>Divcemapper, btrfs, ZFS是snapshoting filesystem，提供block级差异，只能工作在为它们格式化的卷上；</li></ul><h3 id="Bundles"><a href="#Bundles" class="headerlink" title="Bundles"></a>Bundles</h3><ul><li>简单来说，<code>bundles</code>在文件系统上就是一个目录，该目录下包括容器运行时如runC启动容器需要的：配置、元数据、rootfs；</li><li>这个目录是由文件系统驱动程序将镜像的多层文件通过联合挂载的方式以目录的形式呈现给容器运行时的；</li></ul><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><ul><li>Containerd架构上设计被拆成了多个组件，这些组件被组织到了subsystems子系统中；</li><li>这个构架的主要目标是协调<code>bundles</code>的创建和执行；</li></ul><img src="/2024/01/16/cloud-native/containerd/Arch.png" width="70%" style="margin: 0 auto;"><h3 id="subsystems"><a href="#subsystems" class="headerlink" title="subsystems"></a>subsystems</h3><ul><li>用户通过GRPC API与子系统交互，当前containerd提供两个子系统服务：<ul><li>Bundle: Bundle服务为用户提供从磁盘镜像提供bundle和打包bundle为磁盘镜像的能力；</li><li>Runtime: Runtime服务提供执行bundle的能力，包括创建运行时容器；</li></ul></li><li>一般来说，每个子系统都包括一个或多个关联的controller组件来实现该子系统的能力；</li></ul><h3 id="Modules"><a href="#Modules" class="headerlink" title="Modules"></a>Modules</h3><p>除了子系统，Containerd有多个跨子系统边界的组件：</p><ul><li>Executor: 实现容器运行时</li><li>Supervisor: 监控容器状态</li><li>Metadata：存储metadata的图数据库，在contailerd目录中有一个meta.db，是一boltdb文件；</li><li>Content: 具体的blobs文件存储，sha256索引；</li><li>Snapshot: 管理容器镜像的文件系统snapshots，这里的snapshot可以理解为镜像文件目录的联合，如overlay2的联合目录形成容器的rootfs；</li><li>Events: </li><li>Metrics:</li></ul><h3 id="Client-side组件"><a href="#Client-side组件" class="headerlink" title="Client-side组件"></a>Client-side组件</h3><ul><li>一些在客户端实现的组件能力，如镜像的分发接口实现；</li></ul><h3 id="Data-Flow"><a href="#Data-Flow" class="headerlink" title="Data Flow"></a>Data Flow</h3><ul><li>Bundle是containerd的核心，如下流程是bundle的创建过程：<br><img src="/2024/01/16/cloud-native/containerd/bundle-creation.png" alt="Bundle Creation"></li></ul><p>在整个流程中，需要和三个组件进行交互：Distribution, Bundle, Runtime。其中：</p><ul><li>Distribution的作用显然是用于从仓库拉取镜像到content store下，如我本地的<code>/var/lib/containerd/io.containerd.content.v1.content</code>，同时镜像的name和root manifest pointers会被注册到metadata store中，对应我本地的目录<code>/var/lib/containerd/io.containerd.metadata.v1.bolt</code>；</li><li>Bundle会解压镜像，并组装成一个容器运行所需的bundle，镜像的数据从前面的content目录中读取，镜像的layers会被联合mount到snapshot中，对应到我本地的目录：<code>/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs</code>；</li><li>当容器运行所需的rootfs snapshot准备好了，bundle controller会基于镜像的manifest和config内容生成<code>runC</code>容器运行标准所需的配置文件；</li><li>所有这些bundle随后会一起传给Runtime子系统执行，用于创建一个容器；</li></ul><h2 id="Containerd-plugins"><a href="#Containerd-plugins" class="headerlink" title="Containerd plugins"></a>Containerd plugins</h2><h3 id="资料-1"><a href="#资料-1" class="headerlink" title="资料"></a>资料</h3><ul><li><a href="https://github.com/containerd/containerd/blob/main/docs/PLUGINS.md">https://github.com/containerd/containerd/blob/main/docs/PLUGINS.md</a></li></ul><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><ul><li>如前面的Containerd架构图所示，Containerd有很多组件和服务，其中的一些是以插件化的形式提供的，Containerd提供了一对应的插件接口，只要实现这些接口即可自定义扩展这些功能插件；</li><li>这些扩展接口包括：runtime, snapshotter, content store；</li><li>content store service接口：<br><a href="https://pkg.go.dev/github.com/containerd/containerd/v2/api/services/content/v1#ContentServer">https://pkg.go.dev/github.com/containerd/containerd/v2/api/services/content/v1#ContentServer</a></li><li>snapshotter service接口：<br><a href="https://pkg.go.dev/github.com/containerd/containerd/v2/api/services/snapshots/v1#SnapshotsServer">https://pkg.go.dev/github.com/containerd/containerd/v2/api/services/snapshots/v1#SnapshotsServer</a></li><li>diff service接口：<br><a href="https://pkg.go.dev/github.com/containerd/containerd/v2/api/services/diff/v1#DiffServer">https://pkg.go.dev/github.com/containerd/containerd/v2/api/services/diff/v1#DiffServer</a></li><li>配置plugins扩展可以通过两种方式：1. 二进制；2. 配置plugin grpc proxy；</li></ul><h3 id="Proxy-Plugins"><a href="#Proxy-Plugins" class="headerlink" title="Proxy Plugins"></a>Proxy Plugins</h3><ul><li>这里就是配置plugin的GRPC proxy的方式来扩展插件能力；</li><li>containerd plugin GRPC服务监听的是本地的unix socket，配置时需要配置对应的plugin的：name, type, address；</li><li>type类型必须为：<code>snapshot</code>, <code>content</code>, <code>diff</code>；</li></ul><ol><li>编辑containerd配置文件：<code>/etc/containerd/config.toml</code></li><li>增加<code>[proxy_plugins]</code>段，并在段内添加<code>[proxy_plugins.&lt;plugin-name&gt;]</code></li><li>配置如下：<figure class="highlight toml"><table><tr><td class="code"><pre><span class="line"><span class="attr">version</span> = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="section">[proxy_plugins]</span></span><br><span class="line">  <span class="section">[proxy_plugins.&lt;plugin-name&gt;]</span></span><br><span class="line">    <span class="attr">type</span> = <span class="string">&quot;snapshot&quot;</span></span><br><span class="line">    <span class="attr">address</span> = <span class="string">&quot;/var/run/&lt;plugin-name&gt;.sock&quot;</span></span><br></pre></td></tr></table></figure></li></ol><ul><li><p>Every plugin can have its own section using the pattern <code>[plugins.&quot;&lt;plugin type&gt;.&lt;plugin id&gt;&quot;]</code>，如：</p><figure class="highlight toml"><table><tr><td class="code"><pre><span class="line"><span class="attr">version</span> = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="section">[plugins]</span></span><br><span class="line">  <span class="section">[plugins.&quot;io.containerd.monitor.v1.cgroups&quot;]</span></span><br><span class="line">    <span class="attr">no_prometheus</span> = <span class="literal">false</span></span><br></pre></td></tr></table></figure></li><li><p>containerd默认的配置查看：<code>containerd config default</code></p></li><li><p>合并自定义配置后的containerd配置：<code>containerd config dump</code></p></li><li><p>查看当前containerd有的所有plugins列表：<code>ctr plugins ls</code></p></li></ul><h2 id="snapshotter"><a href="#snapshotter" class="headerlink" title="snapshotter"></a>snapshotter</h2><h3 id="资料-2"><a href="#资料-2" class="headerlink" title="资料"></a>资料</h3><ul><li><a href="https://dev.to/napicella/what-is-a-containerd-snapshotters-3eo2">What is a containerd snapshotter?</a></li><li><a href="https://github.com/containerd/containerd/tree/main/docs/snapshotters">https://github.com/containerd/containerd/tree/main/docs/snapshotters</a></li></ul><h3 id="概念与介绍"><a href="#概念与介绍" class="headerlink" title="概念与介绍"></a>概念与介绍</h3><ul><li>snapshotter是类似替换了docker的graph driver的作用；</li><li>snapshotter的接口设计是围绕overlay filesystem和snapshoting filesystem中IO更复杂的snapshoting filesystem设计的，因此在支持overlay的接口上没什么压力；</li><li>如上面的plugins所描述，containerd的snapshotter是以插件化的形式内置，用户可以通过实现snapshotter的GRPC接口实现自定义方案，这就是nydus实现lazy load懒加载技术的基础；</li><li>复用snapshotter plugin机制，官方提供了一种<code>Remote Snapshotter</code>的方案，用于实现类似远程挂载的技术，基于该原理可以实现镜像懒加载实现，类似Slacker的能力；</li><li>官方提供了一种原生的snapshotter实现：<code>native</code>，开插件在合并镜像layer时是直接copy文件，所以磁盘复用率不高，可参考文档：<a href="https://dev.to/napicella/what-is-a-containerd-snapshotters-3eo2">What is a containerd snapshotter?</a>的使用示例，但明显这是最简单的不依赖其它文件系统的实现，在开发测试阶段很实用：<br><img src="/2024/01/16/cloud-native/containerd/native.png" alt="native snapshotter"></li></ul><h3 id="Remote-Snapshotter-and-Stargz-Snapshotter"><a href="#Remote-Snapshotter-and-Stargz-Snapshotter" class="headerlink" title="Remote Snapshotter and Stargz Snapshotter"></a>Remote Snapshotter and Stargz Snapshotter</h3><blockquote><p><a href="https://github.com/containerd/stargz-snapshotter?tab=readme-ov-file">https://github.com/containerd/stargz-snapshotter?tab=readme-ov-file</a><br><a href="https://medium.com/nttlabs/startup-containers-in-lightning-speed-with-lazy-image-distribution-on-containerd-243d94522361">Startup Containers in Lightning Speed with Lazy Image Distribution on Containerd</a></p></blockquote><ul><li>Stargz Snapshotter是containerd的子项目；</li></ul><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><ul><li>谷歌提出了<a href="https://github.com/containerd/stargz-snapshotter?tab=readme-ov-file"><code>Stargz Snapshotter</code></a>就是<code>Remote Snapshotter</code>的一种实现，该方案实现了lazy pulling；<br><img src="/2024/01/16/cloud-native/containerd/stargz-lazypull.png" alt="stargz-lazypull"></li><li>该技术是基于CRFS设计的stargz镜像格式，该格式相对于tar&#x2F;tar.gz格式，最大的不同就是能实现压缩文件的索引，也就是可以直接取到tar.gz压缩文件内的子文件，且取出的子文件也是tar.gz格式，这样基于OCI镜像分发规范的range request特性即可实现lazy pull；</li><li>stargz镜像格式完全兼容tar.gz，因此可以直接用于仅支持OCI镜像格式的容器环境；<br><img src="/2024/01/16/cloud-native/containerd/stargz-format.png" alt="stargz-format"></li></ul><blockquote><p>如我的另一篇介绍<code>nydus</code>的文章说到的，stagz镜像格式是以文件为粒度的lazy pull，共享粒度相对于nydus的block来说更粗；</p></blockquote><ul><li>AWS也实现了一种lazy pull snapshotter方案，声称不需要进行镜像格式的转换，而是生成了一份独立的index artifact(SOCI index)：<a href="https://github.com/awslabs/soci-snapshotter">https://github.com/awslabs/soci-snapshotter</a></li></ul><h2 id="nydus-snapshotter"><a href="#nydus-snapshotter" class="headerlink" title="nydus-snapshotter"></a>nydus-snapshotter</h2><blockquote><p><a href="https://github.com/containerd/nydus-snapshotter">https://github.com/containerd/nydus-snapshotter</a></p></blockquote><ul><li>nydus-snapshotter目前也是containerd的子项目；</li></ul><h2 id="containerd-vs-OCI-O-vs-runC"><a href="#containerd-vs-OCI-O-vs-runC" class="headerlink" title="containerd vs OCI-O vs runC"></a>containerd vs OCI-O vs runC</h2><h3 id="资料-3"><a href="#资料-3" class="headerlink" title="资料"></a>资料</h3><ul><li><a href="https://humalect.com/blog/containerd-vs-docker">https://humalect.com/blog/containerd-vs-docker</a></li><li><a href="https://www.dtstack.com/bbs/article/258">https://www.dtstack.com/bbs/article/258</a></li><li><a href="https://www.rectcircle.cn/posts/containerd-1-quickstart/">https://www.rectcircle.cn/posts/containerd-1-quickstart/</a></li><li><a href="https://github.com/containerd/containerd/blob/v1.7.0/docs/cri/architecture.md">Architecture of The CRI Plugin</a></li></ul><h3 id="层级关系图"><a href="#层级关系图" class="headerlink" title="层级关系图"></a>层级关系图</h3><p><img src="/2024/01/16/cloud-native/containerd/containerd+crio+runc.png" alt="containerd+crio+runc"></p><h3 id="containerd"><a href="#containerd" class="headerlink" title="containerd"></a>containerd</h3><ul><li>由docker捐献给CNCF，作为容器运行时标准；</li><li>作为docker的守护进程，与docker-cli之间是C&#x2F;S架构，docker本身是一整套E2E容器管理方案，具体很多更高层的镜像管理和编排能力，而containerd是一轻量级容器运行时，相对docker而言，其更聚焦于容器运行时的执行和管理；</li><li>containerd以守护进程的方式管理着机器上的所有容器生命周期，包括：<ul><li>镜像下载和存储</li><li>容器rootfs生成</li><li>容器启动和守护</li><li>容器低级存储和网络（接入CNI）</li></ul></li><li>docker是直接通过GRPC API使用containerd的，但k8s是通过CRI GRPC API plugin（原生插件，打包到了containerd进进制中）来与containerd交互，这个plugin的存在也就是说containerd是实现了CRI标准<a href="https://github.com/containerd/containerd/blob/v1.7.0/docs/cri/architecture.md">Architecture of The CRI Plugin</a>；</li><li>containerd是容器运行时的抽象实现层，具体到底层的系统，需要由对应的CRI底层运行时来实现与内核的交互，linux系统下，默认实现是<code>runC</code>；</li></ul><h3 id="docker-vs-containerd"><a href="#docker-vs-containerd" class="headerlink" title="docker vs containerd"></a>docker vs containerd</h3><ul><li>理解了containerd的作用，即可理解下面的docker与containerd的关系：<br><img src="/2024/01/16/cloud-native/containerd/docker+contaierd.png" alt="docker vs containerd"></li><li>docker有两块：<ul><li>docker-cli：命令行工具，提供如docker pull, build, run, exec等高级功能</li><li>containerd：守护进程，负责镜像拉取，容器运行时环境创建（网络，存储，ns），管理容器生命周期；</li><li>runc：底层容器运行时实现，实现了OCI标准；</li></ul></li></ul><h3 id="CRI-O"><a href="#CRI-O" class="headerlink" title="CRI-O"></a>CRI-O</h3><ul><li>开源社区专门为k8s生态打造的CRI标准实现，与k8s seamless integration，与containerd处于同一层，都是实现容器生命周期的管理；</li><li>因其是专对k8s实现的，因此不适用于非k8s的运行时场景；</li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;资料&quot;&gt;&lt;a href=&quot;#资料&quot; class=&quot;headerlink&quot; title=&quot;资料&quot;&gt;&lt;/a&gt;资料&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/containerd/containerd/blob/main/docs/historical/design/architecture.md&quot;&gt;https://github.com/containerd/containerd/blob/main/docs/historical/design/architecture.md&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://blog.frognew.com/2021/05/relearning-container-08.html&quot;&gt;https://blog.frognew.com/2021/05/relearning-container-08.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://blog.frognew.com/2021/06/relearning-container-09.html&quot;&gt;https://blog.frognew.com/2021/06/relearning-container-09.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://blog.mobyproject.org/where-are-containerds-graph-drivers-145fc9b7255&quot;&gt;https://blog.mobyproject.org/where-are-containerds-graph-drivers-145fc9b7255&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;高层架构&lt;br&gt;&lt;img src=&quot;/2024/01/16/cloud-native/containerd/top-arch.png&quot; alt=&quot;Top Arch&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;/a&gt;概念&lt;/h2&gt;&lt;h3 id=&quot;联合挂载&quot;&gt;&lt;a href=&quot;#联合挂载&quot; class=&quot;headerlink&quot; title=&quot;联合挂载&quot;&gt;&lt;/a&gt;联合挂载&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;由overlay filesystem提供的能力，支持将多个文件系统层叠加在一起，且只显示最顶层的文件和目录，OverlayFS是其实现，docker当前默认存储驱动为overlay2，就是基于该文件系统；&lt;/li&gt;
&lt;li&gt;在contaierd中这个联合挂载的roofs视图是由snapshotter准备的snapshots；&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="Cloud Native" scheme="https://so2bin.github.io/tags/Cloud-Native/"/>
    
    <category term="Containerd" scheme="https://so2bin.github.io/tags/Containerd/"/>
    
  </entry>
  
  <entry>
    <title>Lazy Docker Containers</title>
    <link href="https://so2bin.github.io/2024/01/16/papers/lazy-docker/"/>
    <id>https://so2bin.github.io/2024/01/16/papers/lazy-docker/</id>
    <published>2024-01-16T06:55:45.000Z</published>
    <updated>2024-01-16T09:23:20.045Z</updated>
    
    <content type="html"><![CDATA[<h1 id="论文粗读：Slacker-Fast-Distribution-with-Lazy-Docker-Containers"><a href="#论文粗读：Slacker-Fast-Distribution-with-Lazy-Docker-Containers" class="headerlink" title="论文粗读：Slacker: Fast Distribution with Lazy Docker Containers"></a>论文粗读：Slacker: Fast Distribution with Lazy Docker Containers</h1><h2 id="阅读目标"><a href="#阅读目标" class="headerlink" title="阅读目标"></a>阅读目标</h2><ul><li>了解docker容器启动过程的数据特性，以及slacker的设计思路，不对该存储驱动做过细了解。</li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><ul><li>开发了名为<code>HelloBench</code>的工具来分析57个不同的容器应用，分析容器启动过程的IO数据特性和镜像可压缩性，得出结论：在容器的启动过程中，镜像的拉取占76%的时间，但仅读了6.4%的数据。</li><li>基于这些发现，作者设计开发了一种新的docker存储驱动：Slacker，可以用于加速容器的启动速度。</li><li>Slacker采用中心化存储思路，所有的docker workers和registries都共享这些数据。</li><li>Container的启动主要慢在文件系统的瓶颈，相对而言，network, compute, memory资源更快且简单，容器应用需要一个完整的初始化文件系统，包括应用binary，完整的linux系统和依赖包；</li><li>在谷歌的研究论文中，容器应用启动延时变化秀大，中间值一般为25s，且package installation占了80%，其中的一个瓶颈为并发写磁盘的过程；</li><li>提供快启动的好处：<ul><li>应用能快速扩容以应对突发事件（flash-crowd events）；</li><li>集群调度器能以最少的代价频繁执行rebalance nodes；</li><li>应用更新或bug修复能快速发布；</li><li>开发者可以交互式构建和测试分发应用；<span id="more"></span></li></ul></li><li>作者分了两步来尝试解决这个问题：<ol><li>开发了开源工具<code>HelloBench</code>来分析容器启动过程的IO特性和镜像特征，该工具得到如下结论：<ul><li>容器启动阶段，copying package data耗时占了76%；</li><li>这些copy的数据中，仅6.4%的数据是容器启动阶段真实需要的；</li><li>相比于gzip压缩，简单的跨镜像块去重能达到更好的压缩比；</li></ul></li><li>基于这些研究，作者设计开发了Slacker docker存储驱动，该驱动提供了如laziy pull image而不是拉取整个镜像内容，能显著减少网络IO；</li></ol></li><li>基于这些技术，Slacker能提供72倍的镜像pull加速，提供5倍的容器部署周期加速，20倍的应用部署周期加速；</li></ul><h2 id="Docker-Backgroud"><a href="#Docker-Backgroud" class="headerlink" title="Docker Backgroud"></a>Docker Backgroud</h2><h3 id="Version-Control-for-Containers"><a href="#Version-Control-for-Containers" class="headerlink" title="Version Control for Containers"></a>Version Control for Containers</h3><ul><li>cgroups通过6个namepsace来提供多种资源的虚拟化隔离：文件系统挂载点、IPC队列、网络、主机名、进程ID和用户ID；</li><li>docker image是多layer的只读文件集合，layer的拉取在网络上是通过gzip tar文件来传输，在本地的存储格式是由对应的docker存储驱动决定；</li></ul><h3 id="Docker-Storage-Driver-Interface"><a href="#Docker-Storage-Driver-Interface" class="headerlink" title="Docker Storage Driver Interface"></a>Docker Storage Driver Interface</h3><blockquote><p>当前docker默认采用overlay2存储驱动，部分源码可参考我前面的<code>OCI镜像规范</code>文档。</p></blockquote><ul><li>docker容器有两种访问文件系统的方式：1. 通过挂载本机路径到容器中；2. 容器需要访问镜像层的数据，如应用binary, libraries；</li><li>docker是通过存储驱动提供mount point来给容器提供镜像层文件的视图，容器将这些作为其rootfs；</li><li>docker存储驱动需要至少实现如下图的接口：<br><img src="/2024/01/16/papers/lazy-docker/docker-driver-api.png" alt="docker driver api"></li><li>其中的Diff和ApplyDiff分别用于生成新的镜像层tar文件和将层的tar文件联合到容器文件系统中，如下图所示：<br><img src="/2024/01/16/papers/lazy-docker/diff.png" alt="diff"></li><li>联合挂载点提供了底层文件系统的多层目录的聚合视图；</li></ul><blockquote><p>论文中提到的AUFS现在已不是docker的默认存储驱动，现在为overlay2，所以这些内容跳过</p></blockquote><h2 id="Slacker"><a href="#Slacker" class="headerlink" title="Slacker"></a>Slacker</h2><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><ul><li>其构架的核心思路是将镜像&#x2F;容器数据统一存储到一个集中式存储器NFS上，这样容器启动时，仅需要读取的数据才会通过网络拉取，避免了同步整个镜像文件的情况，显著减少网络IO：<br><img src="/2024/01/16/papers/lazy-docker/slacker-arch.png" alt="slacker-arch"></li></ul><blockquote><p>看到这，需要理解的东西基本差不多了，其它一些技术有点过时或不适用于生产，所以不再继续细读。</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;论文粗读：Slacker-Fast-Distribution-with-Lazy-Docker-Containers&quot;&gt;&lt;a href=&quot;#论文粗读：Slacker-Fast-Distribution-with-Lazy-Docker-Containers&quot; class=&quot;headerlink&quot; title=&quot;论文粗读：Slacker: Fast Distribution with Lazy Docker Containers&quot;&gt;&lt;/a&gt;论文粗读：Slacker: Fast Distribution with Lazy Docker Containers&lt;/h1&gt;&lt;h2 id=&quot;阅读目标&quot;&gt;&lt;a href=&quot;#阅读目标&quot; class=&quot;headerlink&quot; title=&quot;阅读目标&quot;&gt;&lt;/a&gt;阅读目标&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;了解docker容器启动过程的数据特性，以及slacker的设计思路，不对该存储驱动做过细了解。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;开发了名为&lt;code&gt;HelloBench&lt;/code&gt;的工具来分析57个不同的容器应用，分析容器启动过程的IO数据特性和镜像可压缩性，得出结论：在容器的启动过程中，镜像的拉取占76%的时间，但仅读了6.4%的数据。&lt;/li&gt;
&lt;li&gt;基于这些发现，作者设计开发了一种新的docker存储驱动：Slacker，可以用于加速容器的启动速度。&lt;/li&gt;
&lt;li&gt;Slacker采用中心化存储思路，所有的docker workers和registries都共享这些数据。&lt;/li&gt;
&lt;li&gt;Container的启动主要慢在文件系统的瓶颈，相对而言，network, compute, memory资源更快且简单，容器应用需要一个完整的初始化文件系统，包括应用binary，完整的linux系统和依赖包；&lt;/li&gt;
&lt;li&gt;在谷歌的研究论文中，容器应用启动延时变化秀大，中间值一般为25s，且package installation占了80%，其中的一个瓶颈为并发写磁盘的过程；&lt;/li&gt;
&lt;li&gt;提供快启动的好处：&lt;ul&gt;
&lt;li&gt;应用能快速扩容以应对突发事件（flash-crowd events）；&lt;/li&gt;
&lt;li&gt;集群调度器能以最少的代价频繁执行rebalance nodes；&lt;/li&gt;
&lt;li&gt;应用更新或bug修复能快速发布；&lt;/li&gt;
&lt;li&gt;开发者可以交互式构建和测试分发应用；&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</summary>
    
    
    
    
    <category term="lazy container" scheme="https://so2bin.github.io/tags/lazy-container/"/>
    
    <category term="docker" scheme="https://so2bin.github.io/tags/docker/"/>
    
  </entry>
  
</feed>
